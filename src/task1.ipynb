{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/ryanschaefer/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from lemminflect import getAllInflections, getAllLemmas\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"framenet_v17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class LexicalUnitClassifier:\n",
    "    def __init__(self, reset_framenet = False, pretrained = True, model_directory = \"../models\"):\n",
    "        self.lu_data = None\n",
    "        self.models = None\n",
    "        self.load_framenet(reset_framenet, model_directory)\n",
    "            \n",
    "        if pretrained:\n",
    "            self.load_trained_models(model_directory)\n",
    "    \n",
    "    # Load relevant framenet data\n",
    "    def load_framenet(self, reset = False, directory = \"../models\"):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Load framenet from file is specified and file exists\n",
    "        if not reset:\n",
    "            filename = os.path.join(directory, \"framenet.json\")\n",
    "            if os.path.isfile(filename):\n",
    "                self.lu_data = json.load(open(filename))\n",
    "                return\n",
    "            else:\n",
    "                print(\"Framenet file not found in directory `{}`. Resetting framenet...\".format(directory))\n",
    "                \n",
    "        # Get all lexical units\n",
    "        lexical_units = fn.lus()\n",
    "        lu_names = list(set(map(lambda x: x.name.split(\".\")[0], lexical_units)))\n",
    "        self.lu_data = { key: { \"frames\": [], \"pos\": [], \"no_frame\": [] } for key in lu_names }\n",
    "        all_sentences = set()\n",
    "        \n",
    "        # Iterate through all lexical units\n",
    "        for lu in tqdm(lexical_units):\n",
    "            lu_split = lu.name.split(\".\")\n",
    "            name = lu_split[0]\n",
    "            pos = lu_split[1]\n",
    "            if pos not in self.lu_data[name][\"pos\"]:\n",
    "                self.lu_data[name][\"pos\"].append(pos)\n",
    "            # Add lexemes if not already defined\n",
    "            if \"lexemes\" not in self.lu_data[name].keys():\n",
    "                self.lu_data[name][\"lexemes\"] = {\n",
    "                    \"consecutive\": all(list(map(lambda x: x[\"breakBefore\"] == \"false\", lu.lexemes)))\n",
    "                }\n",
    "                lu_lemmas = []\n",
    "                for lexeme in lu.lexemes:\n",
    "                    lex = lexeme[\"name\"].lower()\n",
    "                    lu_lemmas.append(\"/\".join(list(set([ x for vals in getAllLemmas(lex).values() for x in vals ] + [ lex ]))))\n",
    "                self.lu_data[name][\"lexemes\"][\"lemmas\"] = lu_lemmas\n",
    "                # Add words in () or [] to lexemes\n",
    "                if \"(\" in name or \"[\" in name:\n",
    "                    # Extract substring in brackets\n",
    "                    tmp_name = name.replace(\"[\", \"(\").replace(\"]\", \")\")\n",
    "                    substr = re.findall(r'\\(.*?\\)', tmp_name)[0]\n",
    "                    # Get all lemmas in tokenized substring\n",
    "                    lemmas = []\n",
    "                    for token in self.nlp(substr):\n",
    "                        lemma = token.lemma_.lower()\n",
    "                        if lemma not in [\"(\", \")\"]:\n",
    "                            # Get all inflections of each lemma\n",
    "                            lemmas.append(\"/\".join(list(set([ x for vals in getAllInflections(lemma).values() for x in vals ] + [ lemma ]))))\n",
    "                    # If lu contains a /, save words as the same lexeme\n",
    "                    if \"/\" in lemmas:\n",
    "                        index = lemmas.index(\"/\")\n",
    "                        lemmas[index - 1] = \"{}/{}\".format(lemmas[index - 1], lemmas[index + 1])\n",
    "                        lemmas.pop(index)\n",
    "                        lemmas.pop(index)\n",
    "                    # Add lemmas to beginning or end based on where the close bracket is\n",
    "                    if tmp_name.index(\")\") == len(tmp_name) - 1:\n",
    "                        self.lu_data[name][\"lexemes\"][\"lemmas\"] = self.lu_data[name][\"lexemes\"][\"lemmas\"] + lemmas\n",
    "                    else:\n",
    "                        self.lu_data[name][\"lexemes\"][\"lemmas\"] = lemmas + self.lu_data[name][\"lexemes\"][\"lemmas\"]\n",
    "                \n",
    "            found = False    \n",
    "            for i, frame in enumerate(self.lu_data[name][\"frames\"]):\n",
    "                # If lu already has frames, add sentences to frame\n",
    "                if frame[\"name\"] == lu.frame.name:\n",
    "                    found = True\n",
    "                    # Iterate through all sentences that include lu\n",
    "                    for sentence in lu.exemplars:\n",
    "                        targetFound, curr2 = self.framenet_sentence(sentence)\n",
    "                        # Add sentence if a target was found\n",
    "                        if targetFound:\n",
    "                            self.lu_data[name][\"frames\"][i][\"sentences\"].append(curr2)\n",
    "                    break\n",
    "                \n",
    "            if not found:\n",
    "                curr = {\n",
    "                    \"name\": lu.frame.name,\n",
    "                    \"sentences\": []\n",
    "                }\n",
    "                # Iterate through all sentences that include lu\n",
    "                for sentence in lu.exemplars:\n",
    "                    all_sentences.add(sentence.text)\n",
    "                    targetFound, curr2 = self.framenet_sentence(sentence)\n",
    "                    # Add sentence if a target was found\n",
    "                    if targetFound:\n",
    "                        curr[\"sentences\"].append(curr2)\n",
    "                # Store sentences\n",
    "                self.lu_data[name][\"frames\"].append(curr)\n",
    "           \n",
    "        # Remove all LUs with fewer than 10 example sentences\n",
    "        for lu in lu_names:\n",
    "            frames = self.lu_data[lu][\"frames\"]\n",
    "            # Get all example sentences and the number of sentences per frame\n",
    "            frame_counts = np.array(list(map(lambda x: len(x[\"sentences\"]), frames)))\n",
    "            if max(frame_counts) < 10:\n",
    "                del self.lu_data[lu]\n",
    "                \n",
    "        # Populate no frame sentences for remaining LUs \n",
    "        self.load_no_frames(list(all_sentences))\n",
    "        # Save framenet data to file\n",
    "        filename = os.path.join(directory, \"framenet.json\")\n",
    "        json.dump(self.lu_data, open(filename, \"w\"), indent = 4)\n",
    "        \n",
    "    def load_no_frames(self, all_sentences):\n",
    "        # Iterate through SpaCy parsed sentences\n",
    "        for i, doc in enumerate(tqdm(self.nlp.pipe(all_sentences), total = len(all_sentences))):\n",
    "            # Get LUs in sentence\n",
    "            lus = self.find_lus(all_sentences[i], doc)\n",
    "            # If LU is not tagged with a frame, add it to no_frame\n",
    "            for lu, tokens in lus:\n",
    "                if all(all_sentences[i] != example[\"text\"] for frame in self.lu_data[lu][\"frames\"] for example in frame[\"sentences\"]):\n",
    "                    self.lu_data[lu][\"no_frame\"].append({ \"text\": all_sentences[i], \"tokens\": tokens })\n",
    "        \n",
    "    def framenet_sentence(self, sentence):\n",
    "        curr2 = {\n",
    "            \"text\": sentence.text,\n",
    "            \"fe\": []\n",
    "        }\n",
    "        \n",
    "        # Extract target and frame elements from sentence\n",
    "        targetFound = False\n",
    "        for aset in sentence.annotationSet:\n",
    "            for layer in aset.layer:\n",
    "                if layer.name == \"Target\":\n",
    "                    if len(layer.label) > 0:\n",
    "                        label = layer.label[0]\n",
    "                        curr2[\"start\"] = label[\"start\"]\n",
    "                        curr2[\"end\"] = label[\"end\"]\n",
    "                        targetFound = True\n",
    "                elif layer.name == \"FE\":\n",
    "                    for label in layer.label:\n",
    "                        if \"start\" in label.keys():\n",
    "                            curr2[\"fe\"].append({\n",
    "                                \"name\": label[\"name\"],\n",
    "                                \"start\": label[\"start\"],\n",
    "                                \"end\": label[\"end\"]\n",
    "                            })\n",
    "                            \n",
    "        return targetFound, curr2\n",
    "                \n",
    "    # Load models and probabilities from files in a directory\n",
    "    def load_trained_models(self, directory = \"../models\"):\n",
    "        self.models = {}\n",
    "        for filename in os.listdir(directory):\n",
    "            comps = filename.split(\".\")\n",
    "            if comps[1] == \"pkl\":\n",
    "                self.models[comps[0]] = pkl.load(open(os.path.join(directory, filename), \"rb\"))\n",
    "            \n",
    "    def pos_tag(self, sentence, doc = None):\n",
    "        # Create spacy parser if one has not already been created\n",
    "        if doc is None:\n",
    "            doc = self.nlp(sentence)\n",
    "            \n",
    "        # Translations from spacy POS tags to framenet suffixes\n",
    "        pos_mapping = {\n",
    "            'ADJ': 'a', # Adjective\n",
    "            'ADV': 'adv', # Adverb\n",
    "            'INTJ': 'intj', # Interjection\n",
    "            'NOUN': 'n', # Noun\n",
    "            'PROPN': 'n', # Proper noun\n",
    "            'VERB': 'v', # Verb\n",
    "            'ADP': 'prep', # Adposition (preposition and postposition)\n",
    "            'AUX': 'v', # Auxiliary verb\n",
    "            'CONJ': 'c', # Conjunction\n",
    "            'CCONJ': 'c', # Coordinating conjunction\n",
    "            'SCONJ': 'scon', # Subordinating conjunction\n",
    "            'DET': 'art', # Determiner (article)\n",
    "            'NUM': 'num', # Numeral\n",
    "            'PART': 'part', # Particle\n",
    "            'PRON': 'pron' # Pronoun\n",
    "        }\n",
    "        \n",
    "        # Convert all spacy tags to framenet suffixes in the text\n",
    "        results = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in pos_mapping.keys():\n",
    "                results.append((token.lemma_.lower(), pos_mapping[token.pos_]))\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    # Get the features of an annotated sentence for training\n",
    "    def annotated_features(self, sentence):\n",
    "        doc = self.nlp(sentence[\"text\"])\n",
    "        \n",
    "        if \"start\" in sentence.keys():\n",
    "            tokens = []\n",
    "            # Get target tokens\n",
    "            for token in doc:\n",
    "                if token.idx == sentence[\"start\"]:\n",
    "                    tokens.append(token)\n",
    "                    break\n",
    "            \n",
    "            # Return features of target if found or None\n",
    "            if len(tokens) == 0:\n",
    "                features = [None, None, None]\n",
    "            else:\n",
    "                features = [\n",
    "                    tokens[0].i / len(doc), tokens[0].dep_, tokens[0].head.lemma_\n",
    "                ]\n",
    "            \n",
    "            return features\n",
    "        else:\n",
    "            return self.prediction_features(sentence[\"text\"], sentence[\"tokens\"], doc)\n",
    "    \n",
    "    # Get the features of an unannotated sentence for prediction\n",
    "    def prediction_features(self, sentence, tokens, doc = None):\n",
    "        # Return None if no tokens given\n",
    "        if len(tokens) == 0:\n",
    "            return [None, None, None]\n",
    "        \n",
    "        # Parse sentence with spacy\n",
    "        if doc == None:\n",
    "            doc = self.nlp(sentence)\n",
    "        \n",
    "        # Return features of tokens\n",
    "        for token in doc:\n",
    "            if token.lemma_.lower() == tokens[0]:\n",
    "                return [token.i / len(doc), token.dep_, token.head.lemma_]\n",
    "                \n",
    "        raise Exception(\"Tokens '{}' not found in sentence '{}'\".format(tokens, sentence))\n",
    "    \n",
    "    # Determine all of the lus that are in a sentence\n",
    "    def find_lus(self, sentence, doc = None):\n",
    "        # Get POS tags\n",
    "        token_pos = self.pos_tag(sentence, doc)\n",
    "        # Iterate through all lus\n",
    "        possible_lus = []\n",
    "        for lu, values in self.lu_data.items():\n",
    "            # Check for multiple consecutive lexemes in the sentence\n",
    "            if values[\"lexemes\"][\"consecutive\"] and len(values[\"lexemes\"][\"lemmas\"]) > 1:\n",
    "                index = 0\n",
    "                tokens = []\n",
    "                # Iterate through tokens\n",
    "                for lemma, _ in token_pos:\n",
    "                    # If token is the next lexeme, increment index\n",
    "                    if any(lemma == w for w in values[\"lexemes\"][\"lemmas\"][index].split(\"/\")):\n",
    "                        index += 1\n",
    "                        tokens.append(lemma)\n",
    "                        # Stop if this was the last lexeme\n",
    "                        if index == len(values[\"lexemes\"][\"lemmas\"]):\n",
    "                            break\n",
    "                    # If first lexeme was found and this was not the next lexeme, stop\n",
    "                    elif index > 0:\n",
    "                        break\n",
    "                # If all lexemes were found, add lu\n",
    "                if index == len(values[\"lexemes\"][\"lemmas\"]):\n",
    "                    found = False\n",
    "                    for i, (_, prev_tokens) in enumerate(possible_lus):\n",
    "                        # Check if all lemmas in lu already in a lu (avoid duplicates)\n",
    "                        if len(prev_tokens) > 1 and all(token in prev_tokens for token in tokens):\n",
    "                            found = True\n",
    "                            break\n",
    "                        # Check if all lemmas in previous lu in lu (prev lu should be replaced)\n",
    "                        elif len(tokens) > 1 and all(token in tokens for token in prev_tokens):\n",
    "                            found = True\n",
    "                            possible_lus[i] = (lu, tokens)\n",
    "                            break\n",
    "                    if not found:\n",
    "                        possible_lus.append((lu, tokens))\n",
    "            # Check for 1 or more nonconsecutive lexemes in the sentence\n",
    "            else:\n",
    "                matchCount = 0\n",
    "                tokens = []\n",
    "                # Iterate through lexemes and tokens\n",
    "                for word in values[\"lexemes\"][\"lemmas\"]:\n",
    "                    for lemma, _ in token_pos:\n",
    "                        # If lexeme is in the sentence, increment count and move on to next lexeme\n",
    "                        if any(w == lemma for w in word.split(\"/\")):\n",
    "                            matchCount += 1\n",
    "                            tokens.append(lemma)\n",
    "                            break\n",
    "                # If all lexemes were found, add lu\n",
    "                if matchCount == len(values[\"lexemes\"][\"lemmas\"]):\n",
    "                    found = False\n",
    "                    for i, (_, prev_tokens) in enumerate(possible_lus):\n",
    "                        # Check if all lemmas in lu already in a lu (avoid duplicates)\n",
    "                        if len(prev_tokens) > 1 and all(token in prev_tokens for token in tokens):\n",
    "                            found = True\n",
    "                            break\n",
    "                        # Check if all lemmas in previous lu in lu (prev lu should be replaced)\n",
    "                        elif len(tokens) > 1 and all(token in tokens for token in prev_tokens):\n",
    "                            found = True\n",
    "                            possible_lus[i] = (lu, tokens)\n",
    "                            break\n",
    "                    if not found:\n",
    "                        possible_lus.append((lu, tokens))\n",
    "        return possible_lus\n",
    "\n",
    "    # Predict which of a lexical unit's possible frames is used in a given sentence\n",
    "    def predict_frame(self, sentence, lu, tokens, doc = None):\n",
    "        # If decision tree is defined, predict with tree\n",
    "        if lu in self.models.keys():\n",
    "            features = self.prediction_features(sentence, tokens, doc)\n",
    "            if None in features:\n",
    "                return None\n",
    "            X = pd.DataFrame([features], columns = [\"location\", \"relation\", \"head\"])\n",
    "            pred_frame = self.models[lu].predict(X)[0]\n",
    "        # Else return no frame\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # Return None if no frame\n",
    "        if pred_frame == \"N/A\":\n",
    "            return None\n",
    "        \n",
    "        return pred_frame\n",
    "    \n",
    "    # Train models and probabilities on framenet example sentences\n",
    "    def fit(self, output_dir = \"../models\", random_state = None):\n",
    "        # Delete old pkl files in output_dir\n",
    "        for file in os.listdir(output_dir):\n",
    "            if file.endswith(\".pkl\"):\n",
    "                os.remove(os.path.join(output_dir, file))\n",
    "            \n",
    "        self.models = {}\n",
    "        # Iterate through lus\n",
    "        for lu, values in tqdm(self.lu_data.items()):\n",
    "            # If lu has more than 1 frame, we need to create probabilities or a model\n",
    "            frames = values[\"frames\"]\n",
    "            # Get all example sentences and the number of sentences per frame\n",
    "            frame_counts = np.array(list(map(lambda x: len(x[\"sentences\"]), frames)))\n",
    "            \n",
    "            # Train a decision tree to predict the frame\n",
    "            # Store sentences and frame labels to train on\n",
    "            data = {\n",
    "                \"sentences\": [],\n",
    "                \"labels\": []\n",
    "            }\n",
    "            for frame in frames:\n",
    "                for sentence in frame[\"sentences\"]:\n",
    "                    data[\"sentences\"].append(sentence)\n",
    "                    data[\"labels\"].append(frame[\"name\"])\n",
    "            # Extract features from annotated example sentences\n",
    "            features = list(map(lambda x: self.annotated_features(x), data[\"sentences\"]))\n",
    "            # Get no frame sentences\n",
    "            no_frames = self.lu_data[lu][\"no_frame\"]\n",
    "            if len(no_frames) > 0:\n",
    "                # Undersample to match most common class\n",
    "                selected_no_frames = np.random.choice(no_frames, size = max(frame_counts))\n",
    "                # Extract features from no frame sentences\n",
    "                for curr in selected_no_frames:\n",
    "                    # Add no frame features to features\n",
    "                    features.append(self.prediction_features(curr[\"text\"], curr[\"tokens\"]))\n",
    "                    data[\"labels\"].append(\"N/A\")\n",
    "            # Store features in data frame\n",
    "            X = pd.DataFrame(features, columns = [\"location\", \"relation\", \"head\"])\n",
    "        \n",
    "            # Pipeline to one-hot-encode categorical features\n",
    "            cat_pipeline = Pipeline([\n",
    "                (\"ohe\", OneHotEncoder(handle_unknown = \"ignore\"))\n",
    "            ])\n",
    "            col_transformer = ColumnTransformer([\n",
    "                (\"cat\", cat_pipeline, [\"relation\", \"head\"])\n",
    "            ])\n",
    "            pipeline = Pipeline([\n",
    "                (\"preprocessing\", col_transformer),\n",
    "                (\"model\", DecisionTreeClassifier(random_state = random_state))\n",
    "            ])\n",
    "            \n",
    "            # Fit decision tree and store in pickle file\n",
    "            pipeline.fit(X, data[\"labels\"])\n",
    "            pkl.dump(pipeline, open(\"{}/{}.pkl\".format(output_dir, lu), \"wb\"))\n",
    "            self.models[lu] = pipeline\n",
    "    \n",
    "    def predict(self, sentences, model_dir = None):\n",
    "        # Load models if not already loaded\n",
    "        if not hasattr(self, \"models\") or self.models is None:\n",
    "            if model_dir is None:\n",
    "                self.load_trained_models()\n",
    "            else:\n",
    "                self.load_trained_models(model_dir)\n",
    "                \n",
    "        predictions = []\n",
    "        # Iterate through sentences to predict\n",
    "        for i, doc in enumerate(tqdm(self.nlp.pipe(sentences), total = len(sentences))):\n",
    "            sentence = sentences[i]\n",
    "            # Identify all lexical units in this sentence\n",
    "            possible_lus = self.find_lus(sentence, doc)\n",
    "            curr = []\n",
    "            # Iterate through lexical units\n",
    "            for lu, tokens in possible_lus:\n",
    "                # Get the possible frames for each lexical unit\n",
    "                possible_frames = self.lu_data[lu][\"frames\"]\n",
    "                # If there is only one frame, assign it to the sentence\n",
    "                if len(possible_frames) == 1:\n",
    "                    curr.append((lu, possible_frames[0][\"name\"]))\n",
    "                # If there is more than one frame, predict which one to use\n",
    "                else:\n",
    "                    frame = self.predict_frame(sentence, lu, tokens, doc)\n",
    "                    if frame is not None:\n",
    "                        curr.append((lu, frame))\n",
    "            # Store predicted frames with the target lexical units for this sentence\n",
    "            predictions.append(curr)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13572/13572 [02:31<00:00, 89.30it/s] \n",
      "100%|██████████| 162279/162279 [1:41:34<00:00, 26.63it/s]    \n"
     ]
    }
   ],
   "source": [
    "model = LexicalUnitClassifier(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/4680 [00:02<35:34,  2.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4680/4680 [1:11:47<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204022 entries, 0 to 204021\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   Lexical Unit    204022 non-null  object\n",
      " 1   Frame           204022 non-null  object\n",
      " 2   Sentence        200751 non-null  object\n",
      " 3   Sentence Count  204022 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 6.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>204022.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>47.642269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>52.379088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>547.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Count\n",
       "count   204022.000000\n",
       "mean        47.642269\n",
       "std         52.379088\n",
       "min          0.000000\n",
       "25%         19.000000\n",
       "50%         33.000000\n",
       "75%         59.000000\n",
       "max        547.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexical Unit</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentence Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>` Not if I can help it . \"</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>And now she took a better look at him , Folly ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>` I could n't help feeling that … well , in yo...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>Yet , looking into those liquid dark eyes , Fr...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>She could n't help the tinge of pink that floo...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204017</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>Dubai 10-28 ( FP ) - Dubai 's Crown Prince She...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204018</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>A Turbo Cat ferry makes a one - hour trip ( 7 ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204019</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>Macau , now the Chinese Special Economic Zone ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204020</th>\n",
       "      <td>zonk out.v</td>\n",
       "      <td>Fall_asleep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204021</th>\n",
       "      <td>zoo.n</td>\n",
       "      <td>Locale_by_use</td>\n",
       "      <td>A popular optional excursion is an hour 's det...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204022 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lexical Unit          Frame  \\\n",
       "0       (can't) help.v   Self_control   \n",
       "1       (can't) help.v   Self_control   \n",
       "2       (can't) help.v   Self_control   \n",
       "3       (can't) help.v   Self_control   \n",
       "4       (can't) help.v   Self_control   \n",
       "...                ...            ...   \n",
       "204017          zone.n         Locale   \n",
       "204018          zone.n         Locale   \n",
       "204019          zone.n         Locale   \n",
       "204020      zonk out.v    Fall_asleep   \n",
       "204021           zoo.n  Locale_by_use   \n",
       "\n",
       "                                                 Sentence  Sentence Count  \n",
       "0                              ` Not if I can help it . \"              11  \n",
       "1       And now she took a better look at him , Folly ...              11  \n",
       "2       ` I could n't help feeling that … well , in yo...              11  \n",
       "3       Yet , looking into those liquid dark eyes , Fr...              11  \n",
       "4       She could n't help the tinge of pink that floo...              11  \n",
       "...                                                   ...             ...  \n",
       "204017  Dubai 10-28 ( FP ) - Dubai 's Crown Prince She...              32  \n",
       "204018  A Turbo Cat ferry makes a one - hour trip ( 7 ...              32  \n",
       "204019  Macau , now the Chinese Special Economic Zone ...              32  \n",
       "204020                                                NaN               0  \n",
       "204021  A popular optional excursion is an hour 's det...               1  \n",
       "\n",
       "[204022 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/lexical_unit_sentences.csv\")\n",
    "\n",
    "display(df.info())\n",
    "display(df.describe())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And now she took a better look at him , Folly could n't help noticing the strong , muscular lines of the broad back under that white shirt .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('look', 'Perception_active'),\n",
       " ('lined', 'Abounding_with'),\n",
       " ('now', 'Temporal_collocation'),\n",
       " (\"(can't) help\", 'Self_control'),\n",
       " ('shirt', 'Clothing'),\n",
       " ('line', 'Roadways'),\n",
       " ('take', 'Removing'),\n",
       " ('under', 'Non-gradable_proximity'),\n",
       " ('notice', 'Becoming_aware'),\n",
       " ('strong', 'Level_of_force_resistance'),\n",
       " ('lining', 'Part_inner_outer'),\n",
       " ('muscular', 'Body_description_holistic')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Face red , chest puffed with indignation , young John would yelp : ` I assure you quite categorically that I never touched the ball . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ball', 'Shapes'),\n",
       " ('young', 'Age'),\n",
       " ('never', 'Negation'),\n",
       " ('puff', 'Ingest_substance'),\n",
       " ('touch', 'Quantified_mass'),\n",
       " ('yelp', 'Communication_noise')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    df[\"Sentence\"][1],\n",
    "    df[df[\"Lexical Unit\"] == \"yelp.v\"].reset_index(drop = True)[\"Sentence\"][0]\n",
    "]\n",
    "predicted_frames = model.predict(sentences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    display(predicted_frames[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(can't) help.v | Self_control\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Lexical Unit\"][1], \"|\", df[\"Frame\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Communication_noise'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Lexical Unit\"] == \"yelp.v\"].reset_index(drop = True)[\"Frame\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200751/200751 [00:06<00:00, 32496.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load example sentences\n",
    "all_sentences = {}\n",
    "for sentence in tqdm(fn.exemplars()):\n",
    "    text = sentence.text\n",
    "    if text in all_sentences.keys():\n",
    "        all_sentences[text].append(( sentence.LU.name, sentence.frame.name ))\n",
    "    else:\n",
    "        all_sentences[text] = [( sentence.LU.name, sentence.frame.name )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171665/171665 [2:06:52<00:00, 22.55it/s]   \n"
     ]
    }
   ],
   "source": [
    "all_text = list(all_sentences.keys())\n",
    "all_predictions = model.predict(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169791/177608 correct found predictions, Accuracy = 0.9559873429124814\n",
      "169791/200751 correct total predictions, Accuracy = 0.8457790994814471\n",
      "871139 extra predictions in 171665 sentences = 5.07464538490665 extra predictions per sentence\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "found_total = 0\n",
    "extra_predictions = 0\n",
    "lu_results = {}\n",
    "for i, (text, frames) in enumerate(all_sentences.items()):\n",
    "    if i == len(all_predictions):\n",
    "        break\n",
    "    extra_predictions += len(list(filter(lambda x: x[1] not in list(map(lambda y: y[1], frames)), all_predictions[i])))\n",
    "    for frame in frames:\n",
    "        total += 1\n",
    "        lu = frame[0].split(\".\")[0]\n",
    "        if lu not in lu_results.keys():\n",
    "            lu_results[lu] = {\n",
    "                \"total\": 1,\n",
    "                \"predictions\": [],\n",
    "                \"texts\": []\n",
    "            }\n",
    "        else:\n",
    "            lu_results[lu][\"total\"] += 1\n",
    "        \n",
    "        predicted_lu = list(filter(lambda x: x[0] == lu, all_predictions[i]))\n",
    "        predicted_frame = list(filter(lambda x: x[1] == frame[1], all_predictions[i]))\n",
    "        predicted = predicted_lu + predicted_frame\n",
    "        if len(predicted) == 0:\n",
    "            pred_frame = None\n",
    "        else:\n",
    "            pred_frame = predicted[0][1]\n",
    "        lu_results[lu][\"predictions\"].append({ \"actual\": frame[1], \"predicted\": pred_frame })\n",
    "        lu_results[lu][\"texts\"].append(all_text[i])\n",
    "        \n",
    "        if len(predicted_frame) > 0:\n",
    "            correct += 1\n",
    "            found_total += 1\n",
    "        elif pred_frame != None:\n",
    "            found_total += 1\n",
    "       \n",
    "print(\"{}/{} correct found predictions, Accuracy = {}\".format(correct, found_total, correct / found_total)) \n",
    "print(\"{}/{} correct total predictions, Accuracy = {}\".format(correct, total, correct / total))\n",
    "print(\"{} extra predictions in {} sentences = {} extra predictions per sentence\".format(extra_predictions, len(all_predictions), extra_predictions / len(all_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_acc = []\n",
    "df_cnts = []\n",
    "df_texts = []\n",
    "for lu, results in lu_results.items():\n",
    "    incorrect = list(filter(lambda x: x[\"actual\"] != x[\"predicted\"], results[\"predictions\"]))\n",
    "    total = results[\"total\"]\n",
    "    correct = total - len(incorrect)\n",
    "    acc = correct / total\n",
    "    df_acc.append(pd.DataFrame({\n",
    "        \"Lexical Unit\": [lu],\n",
    "        \"Correct\": [correct],\n",
    "        \"Total\": [total],\n",
    "        \"Accuracy\": [acc]\n",
    "    }))\n",
    "    combo_cnts = pd.Series.value_counts(results[\"predictions\"])\n",
    "\n",
    "    for i, (combo, count) in enumerate(combo_cnts.items()):\n",
    "        df_cnts.append(pd.DataFrame({\n",
    "            \"Lexical Unit\": [lu],\n",
    "            \"Actual Frame\": [combo[\"actual\"]],\n",
    "            \"Predicted Frame\": [combo[\"predicted\"]],\n",
    "            \"Count\": [count]\n",
    "        }))\n",
    "    \n",
    "    for i, combo in enumerate(results[\"predictions\"]):\n",
    "        df_texts.append(pd.DataFrame({\n",
    "            \"Lexical Unit\": [lu],\n",
    "            \"Actual Frame\": [combo[\"actual\"]],\n",
    "            \"Predicted Frame\": [combo[\"predicted\"]],\n",
    "            \"Sentence\": [results[\"texts\"][i]]\n",
    "        }))\n",
    "    \n",
    "df_acc = pd.concat(df_acc).sort_values([\"Total\"], ascending = False)\n",
    "df_cnts = pd.concat(df_cnts).sort_values([\"Lexical Unit\", \"Actual Frame\", \"Predicted Frame\"])\n",
    "df_texts = pd.concat(df_texts).sort_values([\"Lexical Unit\", \"Actual Frame\", \"Predicted Frame\"])\n",
    "\n",
    "with pd.ExcelWriter(\"../output/Lexical Unit Predictions.xlsx\") as writer:\n",
    "    df_acc.to_excel(writer, sheet_name = \"Accuracy\", index = False)\n",
    "    df_cnts.to_excel(writer, sheet_name = \"Predictions\", index = False)\n",
    "    df_texts.to_excel(writer, sheet_name = \"Sentences\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171665"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
