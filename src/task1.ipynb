{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/ryanschaefer/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from lemminflect import getAllInflections\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import spacy\n",
    "from ipywidgets.widgets.widget_int import IntProgress\n",
    "\n",
    "nltk.download(\"framenet_v17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalUnitClassifier:\n",
    "    def __init__(self, pretrained = True, model_directory = \"../models/lexical_units\"):\n",
    "        self.lu_data = None\n",
    "        self.models = None\n",
    "        self.rules = None\n",
    "        self.load_framenet()\n",
    "            \n",
    "        if pretrained:\n",
    "            self.load_trained_models(model_directory)\n",
    "    \n",
    "    def load_framenet(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        lexical_units = fn.lus()\n",
    "        lu_names = list(set(map(lambda x: x.name, lexical_units)))\n",
    "        self.lu_data = { key: { \"frames\": [] } for key in lu_names }\n",
    "        progress = IntProgress(0, 0, len(lexical_units))\n",
    "        display(progress)\n",
    "        for lu in lexical_units:\n",
    "            name = lu.name\n",
    "            if \"lexemes\" not in self.lu_data[name].keys():\n",
    "                self.lu_data[name][\"lexemes\"] = {\n",
    "                    \"lemmas\": list(map(lambda x: x[\"name\"].lower(), lu.lexemes)),\n",
    "                    \"consecutive\": all(list(map(lambda x: x[\"breakBefore\"] == \"false\", lu.lexemes)))\n",
    "                }\n",
    "                if \"(\" in name or \"[\" in name:\n",
    "                    tmp_name = name.replace(\"[\", \"(\").replace(\"]\", \")\")\n",
    "                    substr = re.findall(r'\\(.*?\\)', tmp_name)[0]\n",
    "                    lemmas = []\n",
    "                    for token in self.nlp(substr):\n",
    "                        lemma = token.lemma_.lower()\n",
    "                        if lemma not in [\"(\", \")\"]:\n",
    "                            lemmas.append(\"/\".join(list(set([ x for vals in getAllInflections(lemma).values() for x in vals ] + [ lemma ]))))\n",
    "                    if \"/\" in lemmas:\n",
    "                        index = lemmas.index(\"/\")\n",
    "                        lemmas[index - 1] = \"{}/{}\".format(lemmas[index - 1], lemmas[index + 1])\n",
    "                        lemmas.pop(index)\n",
    "                        lemmas.pop(index)\n",
    "                    if tmp_name.index(\".\") - tmp_name.index(\")\") == 1:\n",
    "                        self.lu_data[name][\"lexemes\"][\"lemmas\"] = self.lu_data[name][\"lexemes\"][\"lemmas\"] + lemmas\n",
    "                    else:\n",
    "                        self.lu_data[name][\"lexemes\"][\"lemmas\"] = lemmas + self.lu_data[name][\"lexemes\"][\"lemmas\"]\n",
    "                    \n",
    "            curr = {\n",
    "                \"name\": lu.frame.name,\n",
    "                \"sentences\": []\n",
    "            }\n",
    "            for sentence in lu.exemplars:\n",
    "                curr2 = {\n",
    "                    \"text\": sentence.text,\n",
    "                    \"fe\": []\n",
    "                }\n",
    "                \n",
    "                targetFound = False\n",
    "                for aset in sentence.annotationSet:\n",
    "                    for layer in aset.layer:\n",
    "                        if layer.name == \"Target\":\n",
    "                            if len(layer.label) > 0:\n",
    "                                label = layer.label[0]\n",
    "                                curr2[\"start\"] = label[\"start\"]\n",
    "                                curr2[\"end\"] = label[\"end\"]\n",
    "                                targetFound = True\n",
    "                        elif layer.name == \"FE\":\n",
    "                            for label in layer.label:\n",
    "                                if \"start\" in label.keys():\n",
    "                                    curr2[\"fe\"].append({\n",
    "                                        \"name\": label[\"name\"],\n",
    "                                        \"start\": label[\"start\"],\n",
    "                                        \"end\": label[\"end\"]\n",
    "                                    })\n",
    "                if targetFound:\n",
    "                    curr[\"sentences\"].append(curr2)\n",
    "            self.lu_data[name][\"frames\"].append(curr)\n",
    "            progress.value += 1\n",
    "                \n",
    "    def load_trained_models(self, directory = \"../models/lexical_units\"):\n",
    "        self.models = {}\n",
    "        self.rules = json.load(open(\"{}/rules.json\".format(directory)))\n",
    "        for filename in os.listdir(directory):\n",
    "            comps = filename.split(\".\")\n",
    "            if comps[1] == \"pkl\":\n",
    "                self.models[comps[0].replace(\"_\", \".\")] = pkl.load(open(os.path.join(directory, filename), \"rb\"))\n",
    "            \n",
    "    def get_word_lu(self, word, pos):\n",
    "        possible_lus = list(filter(lambda x: x.startswith(\"{}\".format(word.lower())), self.lu_data.keys()))\n",
    "        if len(possible_lus) == 0:\n",
    "            return None\n",
    "        elif len(possible_lus) == 1:\n",
    "            return possible_lus[0]\n",
    "        else:\n",
    "            tmp = word + pos\n",
    "            if tmp in possible_lus:\n",
    "                return tmp\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "    def pos_tag(self, sentence, doc = None):\n",
    "        if doc is None:\n",
    "            doc = self.nlp(sentence)\n",
    "            \n",
    "        pos_mapping = {\n",
    "            'ADJ': 'a',    # Adjective\n",
    "            'ADV': 'adv',  # Adverb\n",
    "            'INTJ': 'intj', # Interjection\n",
    "            'NOUN': 'n',   # Noun\n",
    "            'PROPN': 'n',  # Proper noun\n",
    "            'VERB': 'v',   # Verb\n",
    "            'ADP': 'prep', # Adposition (preposition and postposition)\n",
    "            'AUX': 'v',  # Auxiliary verb\n",
    "            'CONJ': 'c',   # Conjunction\n",
    "            'CCONJ': 'c',  # Coordinating conjunction\n",
    "            'SCONJ': 'scon', # Subordinating conjunction\n",
    "            'DET': 'art',  # Determiner (article)\n",
    "            'NUM': 'num',  # Numeral\n",
    "            'PART': 'part', # Particle\n",
    "            'PRON': 'pron', # Pronoun\n",
    "        }\n",
    "        \n",
    "        results = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in pos_mapping.keys():\n",
    "                results.append((token.lemma_.lower(), pos_mapping[token.pos_]))\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def annotated_features(self, sentence):\n",
    "         # presence/absence of words\n",
    "        # use parse tree (what does structure look like)\n",
    "        # spacy has a dependency parser <-\n",
    "        # look at children edge labels of children in dependency tree\n",
    "        # surrounding words\n",
    "        # word count\n",
    "        # where lu is relative to sentence length <-\n",
    "        # bool vars for certain words being present\n",
    "        # lexical parse tree\n",
    "        # named entities\n",
    "        # POS counts\n",
    "        \n",
    "        doc = self.nlp(sentence[\"text\"])\n",
    "        in_lu = False\n",
    "        tokens = []\n",
    "        for token in doc:\n",
    "            if not in_lu and token.idx == sentence[\"start\"]:\n",
    "                in_lu = True\n",
    "                tokens.append(token)\n",
    "            elif in_lu and token.idx > sentence[\"end\"]:\n",
    "                break\n",
    "        \n",
    "        if len(tokens) == 0:\n",
    "            features = [None, None, None]\n",
    "        else:\n",
    "            features = [\n",
    "                tokens[0].i / len(doc), tokens[0].dep_, tokens[0].head.lemma_\n",
    "            ]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def prediction_features(self, sentence, tokens, doc = None):\n",
    "        if len(tokens) == 0:\n",
    "            return [None, None, None]\n",
    "        \n",
    "        if doc == None:\n",
    "            doc = self.nlp(sentence)\n",
    "        \n",
    "        features = []\n",
    "        for token in doc:\n",
    "            if token.lemma_.lower() == tokens[0]:\n",
    "                return [token.i / len(doc), token.dep_, token.head.lemma_]\n",
    "                \n",
    "        raise Exception(\"Tokens '{}' not found in sentence '{}'\".format(tokens, sentence))\n",
    "    \n",
    "    def find_lus(self, sentence, doc = None):\n",
    "        token_pos = self.pos_tag(sentence, doc)\n",
    "        possible_lus = []\n",
    "        for lu, values in self.lu_data.items():\n",
    "            lu_pos = lu.split(\".\")[-1]\n",
    "            if values[\"lexemes\"][\"consecutive\"] and len(values[\"lexemes\"][\"lemmas\"]) > 1:\n",
    "                index = 0\n",
    "                possible_pos = []\n",
    "                tokens = []\n",
    "                for lemma, pos in token_pos:\n",
    "                    if any(lemma == w for w in values[\"lexemes\"][\"lemmas\"][index].split(\"/\")):\n",
    "                        index += 1\n",
    "                        possible_pos.append(pos)\n",
    "                        tokens.append(lemma)\n",
    "                        if index == len(values[\"lexemes\"][\"lemmas\"]):\n",
    "                            break\n",
    "                    elif index > 0:\n",
    "                        break\n",
    "                if index == len(values[\"lexemes\"][\"lemmas\"]) and lu_pos in possible_pos:\n",
    "                    possible_lus.append((lu, tokens))\n",
    "            else:\n",
    "                matchCount = 0\n",
    "                possible_pos = []\n",
    "                tokens = []\n",
    "                for word in values[\"lexemes\"][\"lemmas\"]:\n",
    "                    for lemma, pos in token_pos:\n",
    "                        if any(w == lemma for w in word.split(\"/\")):\n",
    "                            matchCount += 1\n",
    "                            possible_pos.append(pos)\n",
    "                            tokens.append(lemma)\n",
    "                            break\n",
    "                if matchCount == len(values[\"lexemes\"][\"lemmas\"]) and lu_pos in possible_pos:\n",
    "                    possible_lus.append((lu, tokens))\n",
    "        return possible_lus\n",
    "\n",
    "    def predict_frame(self, sentence, lu, tokens, doc = None):\n",
    "        if lu in self.rules.keys():\n",
    "            probs = self.rules[lu]\n",
    "            pred_frame = np.random.choice(list(probs.keys()), p = list(probs.values()))\n",
    "            return pred_frame\n",
    "        elif lu in self.models.keys():\n",
    "            features = self.prediction_features(sentence, tokens, doc)\n",
    "            if None in features:\n",
    "                return None\n",
    "            X = pd.DataFrame(features, columns = [\"location\", \"relation\", \"head\"])\n",
    "            return self.models[lu].predict(X)[0]\n",
    "        else:\n",
    "            return None\n",
    "            # raise Exception(\"Unknown lexical unit: {}\".format(lu))\n",
    "    \n",
    "    def fit(self, output_dir = \"../models/lexical_units\", random_state = None):\n",
    "        # Delete old pkl files in output_dir\n",
    "        for file in os.listdir(output_dir):\n",
    "            if file.endswith(\".pkl\"):\n",
    "                os.remove(os.path.join(output_dir, file))\n",
    "        \n",
    "        if self.lu_data is None:\n",
    "            self.load_training_data()\n",
    "            \n",
    "        self.rules = {}\n",
    "        self.models = {}\n",
    "        progress = IntProgress(0, 0, len(self.lu_data))\n",
    "        display(progress)\n",
    "        for lu, values in self.lu_data.items():\n",
    "            frames = values[\"frames\"]\n",
    "            if len(frames) > 1:\n",
    "                sentences = list(map(lambda x: x[\"sentences\"], frames))\n",
    "                frame_counts = np.array(list(map(len, sentences)))\n",
    "                \n",
    "                if min(frame_counts) < 10:\n",
    "                    if frame_counts.sum() == 0:\n",
    "                        frame_counts = np.full(len(frame_counts), 1 / len(frame_counts))\n",
    "                    else:\n",
    "                        frame_counts = frame_counts / frame_counts.sum()\n",
    "                    self.rules[lu] = { frames[i][\"name\"]: prob for i, prob in enumerate(frame_counts)}\n",
    "                else:\n",
    "                    data = {\n",
    "                        \"sentences\": [],\n",
    "                        \"labels\": []\n",
    "                    }\n",
    "                    for frame in frames:\n",
    "                        for sentence in frame[\"sentences\"]:\n",
    "                            data[\"sentences\"].append(sentence)\n",
    "                            data[\"labels\"].append(frame[\"name\"])\n",
    "                    features = list(map(lambda x: self.annotated_features(x), data[\"sentences\"]))\n",
    "                    X = pd.DataFrame(features, columns = [\"location\", \"relation\", \"head\"])\n",
    "                \n",
    "                    cat_pipeline = Pipeline([\n",
    "                        (\"ohe\", OneHotEncoder(handle_unknown = \"ignore\"))\n",
    "                    ])\n",
    "                    col_transformer = ColumnTransformer([\n",
    "                        (\"cat\", cat_pipeline, [\"relation\", \"head\"])\n",
    "                    ])\n",
    "                    pipeline = Pipeline([\n",
    "                        (\"preprocessing\", col_transformer),\n",
    "                        (\"model\", DecisionTreeClassifier(random_state = random_state))\n",
    "                    ])\n",
    "                    \n",
    "                    pipeline.fit(X, data[\"labels\"])\n",
    "                    pkl.dump(pipeline, open(\"{}/{}.pkl\".format(output_dir, lu.replace(\".\", \"_\")), \"wb\"))\n",
    "                    self.models[lu] = pipeline\n",
    "            progress.value += 1\n",
    "                    \n",
    "        json.dump(self.rules, open(\"{}/rules.json\".format(output_dir), \"w\"), indent = 4)\n",
    "    \n",
    "    def predict(self, sentences, model_dir = None):\n",
    "        if not hasattr(self, \"models\") or self.models is None:\n",
    "            if model_dir is None:\n",
    "                self.load_trained_models()\n",
    "            else:\n",
    "                self.load_trained_models(model_dir)\n",
    "                \n",
    "        predictions = []\n",
    "        for sentence in sentences:\n",
    "            doc = self.nlp(sentence)\n",
    "            possible_lus = self.find_lus(sentence, doc)\n",
    "            curr = []\n",
    "            for lu, tokens in possible_lus:\n",
    "                possible_frames = self.lu_data[lu][\"frames\"]\n",
    "                if len(possible_frames) == 1:\n",
    "                    curr.append((lu, possible_frames[0][\"name\"]))\n",
    "                else:\n",
    "                    frame = self.predict_frame(sentence, lu, tokens, doc)\n",
    "                    if frame is not None:\n",
    "                        curr.append((lu, frame))\n",
    "            predictions.append(curr)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a70bd85bf7417a80174a3e2b08803d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=13572)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LexicalUnitClassifier(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9131e1117774a80973efd92f5fa76f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=10462)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204022 entries, 0 to 204021\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   Lexical Unit    204022 non-null  object\n",
      " 1   Frame           204022 non-null  object\n",
      " 2   Sentence        200751 non-null  object\n",
      " 3   Sentence Count  204022 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 6.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>204022.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>47.642269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>52.379088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>547.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Count\n",
       "count   204022.000000\n",
       "mean        47.642269\n",
       "std         52.379088\n",
       "min          0.000000\n",
       "25%         19.000000\n",
       "50%         33.000000\n",
       "75%         59.000000\n",
       "max        547.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexical Unit</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentence Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>` Not if I can help it . \"</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>And now she took a better look at him , Folly ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>` I could n't help feeling that … well , in yo...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>Yet , looking into those liquid dark eyes , Fr...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>She could n't help the tinge of pink that floo...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204017</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>Dubai 10-28 ( FP ) - Dubai 's Crown Prince She...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204018</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>A Turbo Cat ferry makes a one - hour trip ( 7 ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204019</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>Macau , now the Chinese Special Economic Zone ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204020</th>\n",
       "      <td>zonk out.v</td>\n",
       "      <td>Fall_asleep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204021</th>\n",
       "      <td>zoo.n</td>\n",
       "      <td>Locale_by_use</td>\n",
       "      <td>A popular optional excursion is an hour 's det...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204022 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lexical Unit          Frame  \\\n",
       "0       (can't) help.v   Self_control   \n",
       "1       (can't) help.v   Self_control   \n",
       "2       (can't) help.v   Self_control   \n",
       "3       (can't) help.v   Self_control   \n",
       "4       (can't) help.v   Self_control   \n",
       "...                ...            ...   \n",
       "204017          zone.n         Locale   \n",
       "204018          zone.n         Locale   \n",
       "204019          zone.n         Locale   \n",
       "204020      zonk out.v    Fall_asleep   \n",
       "204021           zoo.n  Locale_by_use   \n",
       "\n",
       "                                                 Sentence  Sentence Count  \n",
       "0                              ` Not if I can help it . \"              11  \n",
       "1       And now she took a better look at him , Folly ...              11  \n",
       "2       ` I could n't help feeling that … well , in yo...              11  \n",
       "3       Yet , looking into those liquid dark eyes , Fr...              11  \n",
       "4       She could n't help the tinge of pink that floo...              11  \n",
       "...                                                   ...             ...  \n",
       "204017  Dubai 10-28 ( FP ) - Dubai 's Crown Prince She...              32  \n",
       "204018  A Turbo Cat ferry makes a one - hour trip ( 7 ...              32  \n",
       "204019  Macau , now the Chinese Special Economic Zone ...              32  \n",
       "204020                                                NaN               0  \n",
       "204021  A popular optional excursion is an hour 's det...               1  \n",
       "\n",
       "[204022 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/lexical_unit_sentences.csv\")\n",
    "\n",
    "display(df.info())\n",
    "display(df.describe())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And now she took a better look at him , Folly could n't help noticing the strong , muscular lines of the broad back under that white shirt .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('muscular.a', 'Body_description_holistic'),\n",
       " ('of.prep', 'Partitive'),\n",
       " ('look.n', 'Facial_expression'),\n",
       " ('under.prep', 'Non-gradable_proximity'),\n",
       " ('white.a', 'Color'),\n",
       " ('could.v', 'Possibility'),\n",
       " (\"(can't) help.v\", 'Self_control'),\n",
       " ('now.adv', 'Temporal_collocation'),\n",
       " ('broad.a', 'Measurable_attributes'),\n",
       " ('take.v', 'Removing'),\n",
       " ('help.v', 'Assistance'),\n",
       " ('at.prep', 'Locative_relation'),\n",
       " ('strong.a', 'Level_of_force_exertion'),\n",
       " ('line.n', 'Roadways'),\n",
       " ('shirt.n', 'Clothing'),\n",
       " ('notice.v', 'Becoming_aware')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [df[\"Sentence\"][1]]\n",
    "predicted_frames = model.predict(sentences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    display(predicted_frames[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
