{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/ryanschaefer/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from lemminflect import getAllInflections\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import spacy\n",
    "from ipywidgets.widgets.widget_int import IntProgress\n",
    "\n",
    "nltk.download(\"framenet_v17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalUnitClassifier:\n",
    "    def __init__(self, reset_framenet = False, pretrained = True, model_directory = \"../models/lexical_units\"):\n",
    "        self.lu_data = None\n",
    "        self.models = None\n",
    "        self.rules = None\n",
    "        self.load_framenet(reset_framenet, model_directory)\n",
    "            \n",
    "        if pretrained:\n",
    "            self.load_trained_models(model_directory)\n",
    "    \n",
    "    # Load relevant framenet data\n",
    "    def load_framenet(self, reset = False, directory = \"../models/lexical_units\"):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Load framenet from file is specified and file exists\n",
    "        if not reset:\n",
    "            filename = os.path.join(directory, \"framenet.json\")\n",
    "            if os.path.isfile(filename):\n",
    "                self.lu_data = json.load(open(filename))\n",
    "                return\n",
    "            else:\n",
    "                print(\"Framenet file not found in directory `{}`. Resetting framenet...\".format(directory))\n",
    "                \n",
    "        # Get all lexical units\n",
    "        lexical_units = fn.lus()\n",
    "        lu_names = list(set(map(lambda x: x.name, lexical_units)))\n",
    "        self.lu_data = { key: { \"frames\": [] } for key in lu_names }\n",
    "        \n",
    "        # Progress bar\n",
    "        progress = IntProgress(0, 0, len(lexical_units))\n",
    "        display(progress)\n",
    "        \n",
    "        # Iterate through all lexical units\n",
    "        for lu in lexical_units:\n",
    "            name = lu.name\n",
    "            # Add lexemes if not already defined\n",
    "            if \"lexemes\" not in self.lu_data[name].keys():\n",
    "                self.lu_data[name][\"lexemes\"] = {\n",
    "                    \"lemmas\": list(map(lambda x: x[\"name\"].lower(), lu.lexemes)),\n",
    "                    \"consecutive\": all(list(map(lambda x: x[\"breakBefore\"] == \"false\", lu.lexemes)))\n",
    "                }\n",
    "                # Add words in () or [] to lexemes\n",
    "                if \"(\" in name or \"[\" in name:\n",
    "                    # Extract substring in brackets\n",
    "                    tmp_name = name.replace(\"[\", \"(\").replace(\"]\", \")\")\n",
    "                    substr = re.findall(r'\\(.*?\\)', tmp_name)[0]\n",
    "                    # Get all lemmas in tokenized substring\n",
    "                    lemmas = []\n",
    "                    for token in self.nlp(substr):\n",
    "                        lemma = token.lemma_.lower()\n",
    "                        if lemma not in [\"(\", \")\"]:\n",
    "                            # Get all inflections of each lemma\n",
    "                            lemmas.append(\"/\".join(list(set([ x for vals in getAllInflections(lemma).values() for x in vals ] + [ lemma ]))))\n",
    "                    # If lu contains a /, save words as the same lexeme\n",
    "                    if \"/\" in lemmas:\n",
    "                        index = lemmas.index(\"/\")\n",
    "                        lemmas[index - 1] = \"{}/{}\".format(lemmas[index - 1], lemmas[index + 1])\n",
    "                        lemmas.pop(index)\n",
    "                        lemmas.pop(index)\n",
    "                    # Add lemmas to beginning or end based on where the close bracket is\n",
    "                    if tmp_name.index(\".\") - tmp_name.index(\")\") == 1:\n",
    "                        self.lu_data[name][\"lexemes\"][\"lemmas\"] = self.lu_data[name][\"lexemes\"][\"lemmas\"] + lemmas\n",
    "                    else:\n",
    "                        self.lu_data[name][\"lexemes\"][\"lemmas\"] = lemmas + self.lu_data[name][\"lexemes\"][\"lemmas\"]\n",
    "                    \n",
    "            curr = {\n",
    "                \"name\": lu.frame.name,\n",
    "                \"sentences\": []\n",
    "            }\n",
    "            # Iterate through all sentences that include lu\n",
    "            for sentence in lu.exemplars:\n",
    "                curr2 = {\n",
    "                    \"text\": sentence.text,\n",
    "                    \"fe\": []\n",
    "                }\n",
    "                \n",
    "                # Extract target and frame elements from sentence\n",
    "                targetFound = False\n",
    "                for aset in sentence.annotationSet:\n",
    "                    for layer in aset.layer:\n",
    "                        if layer.name == \"Target\":\n",
    "                            if len(layer.label) > 0:\n",
    "                                label = layer.label[0]\n",
    "                                curr2[\"start\"] = label[\"start\"]\n",
    "                                curr2[\"end\"] = label[\"end\"]\n",
    "                                targetFound = True\n",
    "                        elif layer.name == \"FE\":\n",
    "                            for label in layer.label:\n",
    "                                if \"start\" in label.keys():\n",
    "                                    curr2[\"fe\"].append({\n",
    "                                        \"name\": label[\"name\"],\n",
    "                                        \"start\": label[\"start\"],\n",
    "                                        \"end\": label[\"end\"]\n",
    "                                    })\n",
    "                # Add sentence if a target was found\n",
    "                if targetFound:\n",
    "                    curr[\"sentences\"].append(curr2)\n",
    "            # Store sentences\n",
    "            self.lu_data[name][\"frames\"].append(curr)\n",
    "            progress.value += 1\n",
    "            \n",
    "        # Save framenet data to file\n",
    "        filename = os.path.join(directory, \"framenet.json\")\n",
    "        json.dump(self.lu_data, open(filename, \"w\"), indent = 4)\n",
    "                \n",
    "    # Load models and probabilities from files in a directory\n",
    "    def load_trained_models(self, directory = \"../models/lexical_units\"):\n",
    "        self.models = {}\n",
    "        self.rules = json.load(open(\"{}/rules.json\".format(directory)))\n",
    "        for filename in os.listdir(directory):\n",
    "            comps = filename.split(\".\")\n",
    "            if comps[1] == \"pkl\":\n",
    "                self.models[comps[0].replace(\"_\", \".\")] = pkl.load(open(os.path.join(directory, filename), \"rb\"))\n",
    "            \n",
    "    def pos_tag(self, sentence, doc = None):\n",
    "        # Create spacy parser if one has not already been created\n",
    "        if doc is None:\n",
    "            doc = self.nlp(sentence)\n",
    "            \n",
    "        # Translations from spacy POS tags to framenet suffixes\n",
    "        pos_mapping = {\n",
    "            'ADJ': 'a', # Adjective\n",
    "            'ADV': 'adv', # Adverb\n",
    "            'INTJ': 'intj', # Interjection\n",
    "            'NOUN': 'n', # Noun\n",
    "            'PROPN': 'n', # Proper noun\n",
    "            'VERB': 'v', # Verb\n",
    "            'ADP': 'prep', # Adposition (preposition and postposition)\n",
    "            'AUX': 'v', # Auxiliary verb\n",
    "            'CONJ': 'c', # Conjunction\n",
    "            'CCONJ': 'c', # Coordinating conjunction\n",
    "            'SCONJ': 'scon', # Subordinating conjunction\n",
    "            'DET': 'art', # Determiner (article)\n",
    "            'NUM': 'num', # Numeral\n",
    "            'PART': 'part', # Particle\n",
    "            'PRON': 'pron' # Pronoun\n",
    "        }\n",
    "        \n",
    "        # Convert all spacy tags to framenet suffixes in the text\n",
    "        results = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in pos_mapping.keys():\n",
    "                results.append((token.lemma_.lower(), pos_mapping[token.pos_]))\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    # Get the features of an annotated sentence for training\n",
    "    def annotated_features(self, sentence):\n",
    "        doc = self.nlp(sentence[\"text\"])\n",
    "        \n",
    "        if \"start\" in sentence.keys():\n",
    "            in_lu = False\n",
    "            tokens = []\n",
    "            # Get target tokens\n",
    "            for token in doc:\n",
    "                if not in_lu and token.idx == sentence[\"start\"]:\n",
    "                    in_lu = True\n",
    "                    tokens.append(token)\n",
    "                elif in_lu and token.idx > sentence[\"end\"]:\n",
    "                    break\n",
    "            \n",
    "            # Return features of target if found or None\n",
    "            if len(tokens) == 0:\n",
    "                features = [None, None, None]\n",
    "            else:\n",
    "                features = [\n",
    "                    tokens[0].i / len(doc), tokens[0].dep_, tokens[0].head.lemma_\n",
    "                ]\n",
    "            \n",
    "            return features\n",
    "        else:\n",
    "            return self.prediction_features(sentence[\"text\"], sentence[\"tokens\"], doc)\n",
    "    \n",
    "    # Get the features of an unannotated sentence for prediction\n",
    "    def prediction_features(self, sentence, tokens, doc = None):\n",
    "        # Return None if no tokens given\n",
    "        if len(tokens) == 0:\n",
    "            return [None, None, None]\n",
    "        \n",
    "        # Parse sentence with spacy\n",
    "        if doc == None:\n",
    "            doc = self.nlp(sentence)\n",
    "        \n",
    "        # Return features of tokens\n",
    "        for token in doc:\n",
    "            if token.lemma_.lower() == tokens[0]:\n",
    "                return [token.i / len(doc), token.dep_, token.head.lemma_]\n",
    "                \n",
    "        raise Exception(\"Tokens '{}' not found in sentence '{}'\".format(tokens, sentence))\n",
    "    \n",
    "    # Determine all of the lus that are in a sentence\n",
    "    def find_lus(self, sentence, doc = None):\n",
    "        # Get POS tags\n",
    "        token_pos = self.pos_tag(sentence, doc)\n",
    "        # Iterate through all lus\n",
    "        possible_lus = []\n",
    "        for lu, values in self.lu_data.items():\n",
    "            # Get lu POS tag to match with token tags\n",
    "            lu_pos = lu.split(\".\")[-1]\n",
    "            # Check for multiple consecutive lexemes in the sentence\n",
    "            if values[\"lexemes\"][\"consecutive\"] and len(values[\"lexemes\"][\"lemmas\"]) > 1:\n",
    "                index = 0\n",
    "                possible_pos = []\n",
    "                tokens = []\n",
    "                # Iterate through tokens\n",
    "                for lemma, pos in token_pos:\n",
    "                    # If token is the next lexeme, increment index\n",
    "                    if any(lemma == w for w in values[\"lexemes\"][\"lemmas\"][index].split(\"/\")):\n",
    "                        index += 1\n",
    "                        possible_pos.append(pos)\n",
    "                        tokens.append(lemma)\n",
    "                        # Stop if this was the last lexeme\n",
    "                        if index == len(values[\"lexemes\"][\"lemmas\"]):\n",
    "                            break\n",
    "                    # If first lexeme was found and this was not the next lexeme, stop\n",
    "                    elif index > 0:\n",
    "                        break\n",
    "                # If all lexemes were found, add lu\n",
    "                if index == len(values[\"lexemes\"][\"lemmas\"]) and lu_pos in possible_pos:\n",
    "                    # Check if all lemmas in lu already in a lu (avoid duplicates)\n",
    "                    found = False\n",
    "                    for _, prev_tokens in possible_lus:\n",
    "                        if all(token in prev_tokens for token in tokens):\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        possible_lus.append((lu, tokens))\n",
    "            # Check for 1 or more nonconsecutive lexemes in the sentence\n",
    "            else:\n",
    "                matchCount = 0\n",
    "                possible_pos = []\n",
    "                tokens = []\n",
    "                # Iterate through lexemes and tokens\n",
    "                for word in values[\"lexemes\"][\"lemmas\"]:\n",
    "                    for lemma, pos in token_pos:\n",
    "                        # If lexeme is in the sentence, increment count and move on to next lexeme\n",
    "                        if any(w == lemma for w in word.split(\"/\")):\n",
    "                            matchCount += 1\n",
    "                            possible_pos.append(pos)\n",
    "                            tokens.append(lemma)\n",
    "                            break\n",
    "                # If all lexemes were found, add lu\n",
    "                if matchCount == len(values[\"lexemes\"][\"lemmas\"]) and lu_pos in possible_pos:\n",
    "                    # Check if all lemmas in lu already in a lu (avoid duplicates)\n",
    "                    found = False\n",
    "                    for _, prev_tokens in possible_lus:\n",
    "                        if all(token in prev_tokens for token in tokens):\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        possible_lus.append((lu, tokens))\n",
    "        return possible_lus\n",
    "\n",
    "    # Predict which of a lexical unit's possible frames is used in a given sentence\n",
    "    def predict_frame(self, sentence, lu, tokens, doc = None):\n",
    "        # If probabilities are defined, predict based on probabilities\n",
    "        if lu in self.rules.keys():\n",
    "            probs = self.rules[lu]\n",
    "            pred_frame = np.random.choice(list(probs.keys()), p = list(probs.values()))\n",
    "        # If decision tree is defined, predict with tree\n",
    "        elif lu in self.models.keys():\n",
    "            features = self.prediction_features(sentence, tokens, doc)\n",
    "            if None in features:\n",
    "                return None\n",
    "            X = pd.DataFrame([features], columns = [\"location\", \"relation\", \"head\"])\n",
    "            pred_frame = self.models[lu].predict(X)[0]\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        return pred_frame\n",
    "    \n",
    "    # Train models and probabilities on framenet example sentences\n",
    "    def fit(self, output_dir = \"../models/lexical_units\", random_state = None):\n",
    "        # Delete old pkl files in output_dir\n",
    "        for file in os.listdir(output_dir):\n",
    "            if file.endswith(\".pkl\"):\n",
    "                os.remove(os.path.join(output_dir, file))\n",
    "            \n",
    "        self.rules = {}\n",
    "        self.models = {}\n",
    "        progress = IntProgress(0, 0, len(self.lu_data))\n",
    "        display(progress)\n",
    "        # Iterate through lus\n",
    "        for lu, values in self.lu_data.items():\n",
    "            # If lu has more than 1 frame, we need to create probabilities or a model\n",
    "            frames = values[\"frames\"]\n",
    "            if len(frames) > 1:\n",
    "                # Get all example sentences and the number of sentences per frame\n",
    "                sentences = list(map(lambda x: x[\"sentences\"], frames))\n",
    "                frame_counts = np.array(list(map(len, sentences)))\n",
    "                \n",
    "                # If there is a frame with fewer than 10 example sentences, we do not have enough data to train a model\n",
    "                if min(frame_counts) < 10:\n",
    "                    # If there are no example sentences, use a uniform distribution\n",
    "                    if frame_counts.sum() == 0:\n",
    "                        frame_counts = np.full(len(frame_counts), 1 / len(frame_counts))\n",
    "                    # Use proportion of available sentences as probabilities of each frame\n",
    "                    else:\n",
    "                        frame_counts = frame_counts / frame_counts.sum()\n",
    "                    self.rules[lu] = { frames[i][\"name\"]: prob for i, prob in enumerate(frame_counts)}\n",
    "                # If all frames have at least 10 example sentences, we can train a decision tree to predict the frame\n",
    "                else:\n",
    "                    # Store sentences and frame labels to train on\n",
    "                    data = {\n",
    "                        \"sentences\": [],\n",
    "                        \"labels\": []\n",
    "                    }\n",
    "                    for frame in frames:\n",
    "                        for sentence in frame[\"sentences\"]:\n",
    "                            data[\"sentences\"].append(sentence)\n",
    "                            data[\"labels\"].append(frame[\"name\"])\n",
    "                    # Extract features from annotated example sentences\n",
    "                    features = list(map(lambda x: self.annotated_features(x), data[\"sentences\"]))\n",
    "                    X = pd.DataFrame(features, columns = [\"location\", \"relation\", \"head\"])\n",
    "                \n",
    "                    # Pipeline to one-hot-encode categorical features\n",
    "                    cat_pipeline = Pipeline([\n",
    "                        (\"ohe\", OneHotEncoder(handle_unknown = \"ignore\"))\n",
    "                    ])\n",
    "                    col_transformer = ColumnTransformer([\n",
    "                        (\"cat\", cat_pipeline, [\"relation\", \"head\"])\n",
    "                    ])\n",
    "                    pipeline = Pipeline([\n",
    "                        (\"preprocessing\", col_transformer),\n",
    "                        (\"model\", DecisionTreeClassifier(random_state = random_state))\n",
    "                    ])\n",
    "                    \n",
    "                    # Fit decision tree and store in pickle file\n",
    "                    pipeline.fit(X, data[\"labels\"])\n",
    "                    pkl.dump(pipeline, open(\"{}/{}.pkl\".format(output_dir, lu.replace(\".\", \"_\")), \"wb\"))\n",
    "                    self.models[lu] = pipeline\n",
    "            progress.value += 1\n",
    "                    \n",
    "        # Save probabilities to json file\n",
    "        json.dump(self.rules, open(\"{}/rules.json\".format(output_dir), \"w\"), indent = 4)\n",
    "    \n",
    "    def predict(self, sentences, model_dir = None):\n",
    "        # Load models if not already loaded\n",
    "        if not hasattr(self, \"models\") or self.models is None:\n",
    "            if model_dir is None:\n",
    "                self.load_trained_models()\n",
    "            else:\n",
    "                self.load_trained_models(model_dir)\n",
    "                \n",
    "        predictions = []\n",
    "        # Iterate through sentences to predict\n",
    "        for sentence in sentences:\n",
    "            # Parse sentence with spacy\n",
    "            doc = self.nlp(sentence)\n",
    "            # Identify all lexical units in this sentence\n",
    "            possible_lus = self.find_lus(sentence, doc)\n",
    "            curr = []\n",
    "            # Iterate through lexical units\n",
    "            for lu, tokens in possible_lus:\n",
    "                # Get the possible frames for each lexical unit\n",
    "                possible_frames = self.lu_data[lu][\"frames\"]\n",
    "                # If there is only one frame, assign it to the sentence\n",
    "                if len(possible_frames) == 1:\n",
    "                    curr.append((lu, possible_frames[0][\"name\"]))\n",
    "                # If there is more than one frame, predict which one to use\n",
    "                else:\n",
    "                    frame = self.predict_frame(sentence, lu, tokens, doc)\n",
    "                    if frame is not None:\n",
    "                        curr.append((lu, frame))\n",
    "            # Store predicted frames with the target lexical units for this sentence\n",
    "            predictions.append(curr)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9f4431079043fc9461a401ab37db3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=13572)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LexicalUnitClassifier(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345190c54259488597f91d358b78f6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=10462)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204022 entries, 0 to 204021\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   Lexical Unit    204022 non-null  object\n",
      " 1   Frame           204022 non-null  object\n",
      " 2   Sentence        200751 non-null  object\n",
      " 3   Sentence Count  204022 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 6.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>204022.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>47.642269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>52.379088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>547.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Count\n",
       "count   204022.000000\n",
       "mean        47.642269\n",
       "std         52.379088\n",
       "min          0.000000\n",
       "25%         19.000000\n",
       "50%         33.000000\n",
       "75%         59.000000\n",
       "max        547.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexical Unit</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentence Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>` Not if I can help it . \"</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>And now she took a better look at him , Folly ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>` I could n't help feeling that … well , in yo...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>Yet , looking into those liquid dark eyes , Fr...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>She could n't help the tinge of pink that floo...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204017</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>Dubai 10-28 ( FP ) - Dubai 's Crown Prince She...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204018</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>A Turbo Cat ferry makes a one - hour trip ( 7 ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204019</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>Macau , now the Chinese Special Economic Zone ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204020</th>\n",
       "      <td>zonk out.v</td>\n",
       "      <td>Fall_asleep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204021</th>\n",
       "      <td>zoo.n</td>\n",
       "      <td>Locale_by_use</td>\n",
       "      <td>A popular optional excursion is an hour 's det...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204022 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lexical Unit          Frame  \\\n",
       "0       (can't) help.v   Self_control   \n",
       "1       (can't) help.v   Self_control   \n",
       "2       (can't) help.v   Self_control   \n",
       "3       (can't) help.v   Self_control   \n",
       "4       (can't) help.v   Self_control   \n",
       "...                ...            ...   \n",
       "204017          zone.n         Locale   \n",
       "204018          zone.n         Locale   \n",
       "204019          zone.n         Locale   \n",
       "204020      zonk out.v    Fall_asleep   \n",
       "204021           zoo.n  Locale_by_use   \n",
       "\n",
       "                                                 Sentence  Sentence Count  \n",
       "0                              ` Not if I can help it . \"              11  \n",
       "1       And now she took a better look at him , Folly ...              11  \n",
       "2       ` I could n't help feeling that … well , in yo...              11  \n",
       "3       Yet , looking into those liquid dark eyes , Fr...              11  \n",
       "4       She could n't help the tinge of pink that floo...              11  \n",
       "...                                                   ...             ...  \n",
       "204017  Dubai 10-28 ( FP ) - Dubai 's Crown Prince She...              32  \n",
       "204018  A Turbo Cat ferry makes a one - hour trip ( 7 ...              32  \n",
       "204019  Macau , now the Chinese Special Economic Zone ...              32  \n",
       "204020                                                NaN               0  \n",
       "204021  A popular optional excursion is an hour 's det...               1  \n",
       "\n",
       "[204022 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/lexical_unit_sentences.csv\")\n",
    "\n",
    "display(df.info())\n",
    "display(df.describe())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And now she took a better look at him , Folly could n't help noticing the strong , muscular lines of the broad back under that white shirt .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"(can't) help.v\", 'Self_control'),\n",
       " ('take.v', 'Removing'),\n",
       " ('muscular.a', 'Body_description_holistic'),\n",
       " ('line.n', 'Roadways'),\n",
       " ('strong.a', 'Judgment_of_intensity'),\n",
       " ('now.adv', 'Temporal_collocation'),\n",
       " ('at.prep', 'Spatial_co-location'),\n",
       " ('broad.a', 'Dimension'),\n",
       " ('notice.v', 'Becoming_aware'),\n",
       " ('look.n', 'Perception_active'),\n",
       " ('under.prep', 'Non-gradable_proximity'),\n",
       " ('shirt.n', 'Clothing'),\n",
       " ('of.prep', 'Partitive'),\n",
       " ('white.a', 'Color')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [df[\"Sentence\"][1]]\n",
    "predicted_frames = model.predict(sentences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    display(predicted_frames[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face red , chest puffed with indignation , young John would yelp : ` I assure you quite categorically that I never touched the ball . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('never.adv', 'Negation'),\n",
       " ('young.a', 'Age'),\n",
       " ('would.v', 'Likelihood'),\n",
       " ('face.v', 'Confronting_problem'),\n",
       " ('touch.v', 'Impact'),\n",
       " ('assure.v', 'Telling'),\n",
       " ('with.prep', 'Accompaniment'),\n",
       " ('chest.n', 'Body_parts'),\n",
       " ('ball.n', 'Shapes'),\n",
       " ('yelp.v', 'Communication_noise'),\n",
       " ('puff.v', 'Ingest_substance')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [df[df[\"Lexical Unit\"] == \"yelp.v\"].reset_index(drop = True)[\"Sentence\"][0]]\n",
    "predicted_frames = model.predict(sentences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    display(predicted_frames[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexical Unit</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentence Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203663</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>Face red , chest puffed with indignation , you...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203664</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>He leaned backwards , digging in his heels , y...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203665</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>` Ouch , \" she yelped .</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203666</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>` Christ , it 's one them white buggers , \" ye...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203667</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>` Christ , he got rid of the blanket , \" yelpe...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203668</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>` You 've got to be kidding ! \" yelped Margare...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203669</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>` Yeah ! \" yelps Paul , furiously .</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203670</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>` It is not , \" yelped Auguste , wishing Egber...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203671</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>Have you prepared them ? \" yelped Auguste .</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203672</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>` I did n't mean to do that , \" she yelped Yan...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203673</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Communication_noise</td>\n",
       "      <td>` Pay me ? \" he yelped with youthful offence .</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203674</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>She flung herself on Wexford , yelping freneti...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203675</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>K!sdra yelped involuntarily as the tip of the ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203676</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>A searching , frantic hand savagely grasped Ma...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203677</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>It was Magnus who discovered the girl first ; ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203678</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>The wolves yelped uncontrollably , throwing th...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203679</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>He yelped shrilly and dropped his guard just s...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203680</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>Her heart flew to her mouth as she yelped and ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203681</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>Then , all of a sudden , there was a sort of b...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203682</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>It could be threatening to the group or at lea...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203683</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>She yelped and squealed , working her voice to...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203684</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>Steve yelped and Newman grabbed the gun he was...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203685</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>My dogs were straining against the sledge now ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203686</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>Next morning , she yawned , wakened , saw his ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203687</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>At the far end of the table Nadirpur almost ye...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203688</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>Farrar yelped at the sound of grinding bones ,...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203689</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>Cranston turned and quietly cursed as the madm...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203690</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>Her foot connected hard with Benton 's right s...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203691</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>Despite herself , Piper yelped with surprise .</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203692</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>The boy withdrew , yelping with frustration , ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203693</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>Some whooped and yelped like savages , others ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203694</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>She burnt her ear , and yelped .</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203695</th>\n",
       "      <td>yelp.v</td>\n",
       "      <td>Make_noise</td>\n",
       "      <td>Her teeth sank into the hand at her mouth and ...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Lexical Unit                Frame  \\\n",
       "203663       yelp.v  Communication_noise   \n",
       "203664       yelp.v  Communication_noise   \n",
       "203665       yelp.v  Communication_noise   \n",
       "203666       yelp.v  Communication_noise   \n",
       "203667       yelp.v  Communication_noise   \n",
       "203668       yelp.v  Communication_noise   \n",
       "203669       yelp.v  Communication_noise   \n",
       "203670       yelp.v  Communication_noise   \n",
       "203671       yelp.v  Communication_noise   \n",
       "203672       yelp.v  Communication_noise   \n",
       "203673       yelp.v  Communication_noise   \n",
       "203674       yelp.v           Make_noise   \n",
       "203675       yelp.v           Make_noise   \n",
       "203676       yelp.v           Make_noise   \n",
       "203677       yelp.v           Make_noise   \n",
       "203678       yelp.v           Make_noise   \n",
       "203679       yelp.v           Make_noise   \n",
       "203680       yelp.v           Make_noise   \n",
       "203681       yelp.v           Make_noise   \n",
       "203682       yelp.v           Make_noise   \n",
       "203683       yelp.v           Make_noise   \n",
       "203684       yelp.v           Make_noise   \n",
       "203685       yelp.v           Make_noise   \n",
       "203686       yelp.v           Make_noise   \n",
       "203687       yelp.v           Make_noise   \n",
       "203688       yelp.v           Make_noise   \n",
       "203689       yelp.v           Make_noise   \n",
       "203690       yelp.v           Make_noise   \n",
       "203691       yelp.v           Make_noise   \n",
       "203692       yelp.v           Make_noise   \n",
       "203693       yelp.v           Make_noise   \n",
       "203694       yelp.v           Make_noise   \n",
       "203695       yelp.v           Make_noise   \n",
       "\n",
       "                                                 Sentence  Sentence Count  \n",
       "203663  Face red , chest puffed with indignation , you...              11  \n",
       "203664  He leaned backwards , digging in his heels , y...              11  \n",
       "203665                           ` Ouch , \" she yelped .               11  \n",
       "203666  ` Christ , it 's one them white buggers , \" ye...              11  \n",
       "203667  ` Christ , he got rid of the blanket , \" yelpe...              11  \n",
       "203668  ` You 've got to be kidding ! \" yelped Margare...              11  \n",
       "203669               ` Yeah ! \" yelps Paul , furiously .               11  \n",
       "203670  ` It is not , \" yelped Auguste , wishing Egber...              11  \n",
       "203671       Have you prepared them ? \" yelped Auguste .               11  \n",
       "203672  ` I did n't mean to do that , \" she yelped Yan...              11  \n",
       "203673    ` Pay me ? \" he yelped with youthful offence .               11  \n",
       "203674  She flung herself on Wexford , yelping freneti...              22  \n",
       "203675  K!sdra yelped involuntarily as the tip of the ...              22  \n",
       "203676  A searching , frantic hand savagely grasped Ma...              22  \n",
       "203677  It was Magnus who discovered the girl first ; ...              22  \n",
       "203678  The wolves yelped uncontrollably , throwing th...              22  \n",
       "203679  He yelped shrilly and dropped his guard just s...              22  \n",
       "203680  Her heart flew to her mouth as she yelped and ...              22  \n",
       "203681  Then , all of a sudden , there was a sort of b...              22  \n",
       "203682  It could be threatening to the group or at lea...              22  \n",
       "203683  She yelped and squealed , working her voice to...              22  \n",
       "203684  Steve yelped and Newman grabbed the gun he was...              22  \n",
       "203685  My dogs were straining against the sledge now ...              22  \n",
       "203686  Next morning , she yawned , wakened , saw his ...              22  \n",
       "203687  At the far end of the table Nadirpur almost ye...              22  \n",
       "203688  Farrar yelped at the sound of grinding bones ,...              22  \n",
       "203689  Cranston turned and quietly cursed as the madm...              22  \n",
       "203690  Her foot connected hard with Benton 's right s...              22  \n",
       "203691    Despite herself , Piper yelped with surprise .               22  \n",
       "203692  The boy withdrew , yelping with frustration , ...              22  \n",
       "203693  Some whooped and yelped like savages , others ...              22  \n",
       "203694                  She burnt her ear , and yelped .               22  \n",
       "203695  Her teeth sank into the hand at her mouth and ...              22  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Lexical Unit\"] == \"yelp.v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
