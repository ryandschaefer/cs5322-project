{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from difflib import get_close_matches\n",
    "import json\n",
    "from lemminflect import getAllLemmas, getAllInflections\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204022 entries, 0 to 204021\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   Lexical Unit    204022 non-null  object\n",
      " 1   Frame           204022 non-null  object\n",
      " 2   Sentence        200751 non-null  object\n",
      " 3   Sentence Count  204022 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 6.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>204022.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>47.642269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>52.379088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>547.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Count\n",
       "count   204022.000000\n",
       "mean        47.642269\n",
       "std         52.379088\n",
       "min          0.000000\n",
       "25%         19.000000\n",
       "50%         33.000000\n",
       "75%         59.000000\n",
       "max        547.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexical Unit</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentence Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>` Not if I can help it . \"</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>And now she took a better look at him , Folly ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>` I could n't help feeling that … well , in yo...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>Yet , looking into those liquid dark eyes , Fr...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(can't) help.v</td>\n",
       "      <td>Self_control</td>\n",
       "      <td>She could n't help the tinge of pink that floo...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204017</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>Dubai 10-28 ( FP ) - Dubai 's Crown Prince She...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204018</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>A Turbo Cat ferry makes a one - hour trip ( 7 ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204019</th>\n",
       "      <td>zone.n</td>\n",
       "      <td>Locale</td>\n",
       "      <td>Macau , now the Chinese Special Economic Zone ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204020</th>\n",
       "      <td>zonk out.v</td>\n",
       "      <td>Fall_asleep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204021</th>\n",
       "      <td>zoo.n</td>\n",
       "      <td>Locale_by_use</td>\n",
       "      <td>A popular optional excursion is an hour 's det...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204022 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lexical Unit          Frame  \\\n",
       "0       (can't) help.v   Self_control   \n",
       "1       (can't) help.v   Self_control   \n",
       "2       (can't) help.v   Self_control   \n",
       "3       (can't) help.v   Self_control   \n",
       "4       (can't) help.v   Self_control   \n",
       "...                ...            ...   \n",
       "204017          zone.n         Locale   \n",
       "204018          zone.n         Locale   \n",
       "204019          zone.n         Locale   \n",
       "204020      zonk out.v    Fall_asleep   \n",
       "204021           zoo.n  Locale_by_use   \n",
       "\n",
       "                                                 Sentence  Sentence Count  \n",
       "0                              ` Not if I can help it . \"              11  \n",
       "1       And now she took a better look at him , Folly ...              11  \n",
       "2       ` I could n't help feeling that … well , in yo...              11  \n",
       "3       Yet , looking into those liquid dark eyes , Fr...              11  \n",
       "4       She could n't help the tinge of pink that floo...              11  \n",
       "...                                                   ...             ...  \n",
       "204017  Dubai 10-28 ( FP ) - Dubai 's Crown Prince She...              32  \n",
       "204018  A Turbo Cat ferry makes a one - hour trip ( 7 ...              32  \n",
       "204019  Macau , now the Chinese Special Economic Zone ...              32  \n",
       "204020                                                NaN               0  \n",
       "204021  A popular optional excursion is an hour 's det...               1  \n",
       "\n",
       "[204022 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/lexical_unit_sentences.csv\")\n",
    "\n",
    "display(df.info())\n",
    "display(df.describe())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 `\n",
      "1  \n",
      "2 N\n",
      "3 o\n",
      "4 t\n",
      "5  \n",
      "6 i\n",
      "7 f\n",
      "8  \n",
      "9 I\n",
      "10  \n",
      "11 c\n",
      "12 a\n",
      "13 n\n",
      "14  \n",
      "15 h\n",
      "16 e\n",
      "17 l\n",
      "18 p\n",
      "19  \n",
      "20 i\n",
      "21 t\n",
      "22  \n",
      "23 .\n",
      "24  \n",
      "25 \"\n"
     ]
    }
   ],
   "source": [
    "for lu in fn.lus():\n",
    "    for i, char in enumerate(lu.exemplars[0].text):\n",
    "        print(i, char)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(can't) help.v\n",
      "exemplar sentence (4166707):\n",
      "[corpID] 111\n",
      "[docID] 421\n",
      "[paragNo] 1609\n",
      "[sentNo] 1\n",
      "[aPos] 78308214\n",
      "\n",
      "[LU] (16601) (can't) help.v in Self_control\n",
      "\n",
      "[frame] (2651) Self_control\n",
      "\n",
      "[annotationSet] 2 annotation sets\n",
      "\n",
      "[POS] 0 tags\n",
      "\n",
      "[POS_tagset] PENN\n",
      "\n",
      "[GF] 2 relations\n",
      "\n",
      "[PT] 2 phrases\n",
      "\n",
      "[text] + [Target] + [FE]\n",
      "\n",
      "` Not if I can help it . \"\n",
      "         -     **** --\n",
      "         A          Ev\n",
      " (A=Agent, Ev=Event)\n",
      "\n",
      "\n",
      "\n",
      "[cBy] MJE\n",
      "[end] 18\n",
      "[start] 15\n",
      "[name] Target\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lu in fn.lus():\n",
    "    print(lu.name)\n",
    "    print(lu.exemplars[0])\n",
    "    for aset in lu.exemplars[0].annotationSet:\n",
    "        for layer in aset.layer:\n",
    "            if layer.name == \"Target\":\n",
    "                for l in layer.label:\n",
    "                    print(l)\n",
    "        print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v       83906\n",
       "n       79057\n",
       "a       34355\n",
       "prep     2991\n",
       "adv      2150\n",
       "scon      760\n",
       "num       353\n",
       "art       269\n",
       "idio      124\n",
       "c          51\n",
       "intj        5\n",
       "pron        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffixes = []\n",
    "for lu in df[\"Lexical Unit\"]:\n",
    "    suffixes.append(lu.split(\".\")[-1])\n",
    "    \n",
    "pd.Series.value_counts(suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lu in fn.lus():\n",
    "    lu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexicalUnitClassifier:\n",
    "    def __init__(\n",
    "        self, load_training = True, pretrained = True, \n",
    "        training_filename = \"../datasets/lexical_unit_sentences.csv\",\n",
    "        model_directory = \"../models/lexical_units\"\n",
    "    ):\n",
    "        self.df = None\n",
    "        self.models = None\n",
    "        self.rules = None\n",
    "        self.load_framenet()\n",
    "    \n",
    "        if load_training:\n",
    "            self.load_training_data(training_filename)\n",
    "            \n",
    "        if pretrained:\n",
    "            self.load_trained_models(model_directory)\n",
    "            \n",
    "    def load_framenet(self):\n",
    "        nltk.download(\"framenet_v17\")\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        lexical_units = fn.lus()\n",
    "        lu_names = list(set(map(lambda x: x.name.replace(\".\", \"_\"), lexical_units)))\n",
    "        self.lu_frames = { key: [] for key in lu_names }\n",
    "        for lu in lexical_units:\n",
    "            name = str.replace(lu.name, \".\", \"_\")\n",
    "            if name not in self.lu_frames[name]:\n",
    "                self.lu_frames[name].append(lu.frame.name)\n",
    "                self.lu_frames[name] = list(sorted(self.lu_frames[name]))\n",
    "    \n",
    "    def load_training_data(self, filename = \"../datasets/lexical_unit_sentences.csv\"):\n",
    "        self.df = pd.read_csv(filename)\n",
    "        self.df[\"Lexical Unit\"] = self.df[\"Lexical Unit\"].str.replace(\".\", \"_\")\n",
    "        \n",
    "    def load_trained_models(self, directory = \"../models/lexical_units\"):\n",
    "        self.models = {}\n",
    "        self.rules = json.load(open(\"{}/rules.json\".format(directory)))\n",
    "        for filename in os.listdir(directory):\n",
    "            comps = filename.split(\".\")\n",
    "            if comps[1] == \"pkl\":\n",
    "                self.models[comps[0]] = pkl.load(open(os.path.join(directory, filename), \"rb\"))\n",
    "            \n",
    "    def get_word_lu(self, word, pos):\n",
    "        possible_lus = list(filter(lambda x: x.startswith(\"{}\".format(word.lower())), self.lu_frames.keys()))\n",
    "        if len(possible_lus) == 0:\n",
    "            return None\n",
    "        elif len(possible_lus) == 1:\n",
    "            return possible_lus[0]\n",
    "        else:\n",
    "            tmp = word + pos\n",
    "            if tmp in possible_lus:\n",
    "                return tmp\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "    def pos_tag(self, sentence, doc = None):\n",
    "        if doc is None:\n",
    "            doc = self.nlp(sentence)\n",
    "            \n",
    "        pos_mapping = {\n",
    "            'ADJ': 'a',    # Adjective\n",
    "            'ADV': 'adv',  # Adverb\n",
    "            'INTJ': 'intj', # Interjection\n",
    "            'NOUN': 'n',   # Noun\n",
    "            'PROPN': 'n',  # Proper noun\n",
    "            'VERB': 'v',   # Verb\n",
    "            'ADP': 'prep', # Adposition (preposition and postposition)\n",
    "            'AUX': 'v',  # Auxiliary verb\n",
    "            'CONJ': 'c',   # Conjunction\n",
    "            'CCONJ': 'c',  # Coordinating conjunction\n",
    "            'SCONJ': 'scon', # Subordinating conjunction\n",
    "            'DET': 'art',  # Determiner (article)\n",
    "            'NUM': 'num',  # Numeral\n",
    "            'PART': 'part', # Particle\n",
    "            'PRON': 'pron', # Pronoun\n",
    "        }\n",
    "        \n",
    "        results = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in pos_mapping.keys():\n",
    "                results.append((token.text.lower(), pos_mapping[token.pos_]))\n",
    "                \n",
    "        return results\n",
    "            \n",
    "    def process_sentence(self, sentence, doc = None):\n",
    "        # presence/absence of words\n",
    "        # use parse tree (what does structure look like)\n",
    "        # spacy has a dependency parser <-\n",
    "        # look at children edge labels of children in dependency tree\n",
    "        # surrounding words\n",
    "        # word count\n",
    "        # where lu is relative to sentence length <-\n",
    "        # bool vars for certain words being present\n",
    "        # lexical parse tree\n",
    "        # named entities\n",
    "        # POS counts\n",
    "        \n",
    "        if doc is None:\n",
    "            doc = self.nlp(sentence)\n",
    "        length = len(doc)\n",
    "        results = {}\n",
    "        first = None\n",
    "        for i, token in enumerate(doc):\n",
    "            results[token.lemma_.lower()] = [\n",
    "                i / length, token.dep_, token.head.lemma_\n",
    "            ]\n",
    "            \n",
    "            if token.dep_ in [\"ROOT\", \"ccomp\"]:\n",
    "                first = token\n",
    "            elif token.dep_ == \"prt\" and first != None:\n",
    "                results[\"{} {}\".format(first.lemma_.lower(), token.lemma_.lower())] = [\n",
    "                    i / length, doc[i - 1].dep_, doc[i - 1].head.lemma_\n",
    "                ]\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def match_lemmas(self, sentence, lu):\n",
    "        real_word =  \" \".join(lu.split(\"_\")[:-1])\n",
    "\n",
    "        # creates set to get all lemma options and fills with possibles keys for lemmas\n",
    "        unique_lemmas = set()\n",
    "        for token in self.nlp(real_word):\n",
    "            unique_lemmas.add(token.text)\n",
    "            unique_lemmas.add(token.lemma_)\n",
    "        \n",
    "        if real_word.count(\" \") == 0:\n",
    "            for lemma in getAllLemmas(real_word).values():\n",
    "                for word in lemma:\n",
    "                    unique_lemmas.add(word.lower())\n",
    "        else:\n",
    "            for w in real_word.split(\" \"):\n",
    "                for lemma in getAllLemmas(w).values():\n",
    "                    for word in lemma:\n",
    "                        unique_lemmas.add(word.lower())\n",
    "\n",
    "        # gets all iterations of the base lemmas to see all lemma options\n",
    "        for lem in list(unique_lemmas):\n",
    "            for infl in getAllInflections(lem).values():\n",
    "                for word in infl:\n",
    "                    unique_lemmas.add(word.lower())\n",
    "                    for lemma in getAllLemmas(word).values():\n",
    "                        for w in lemma:\n",
    "                            unique_lemmas.add(w.lower())\n",
    "\n",
    "        # gets the features for only the target lemma within the sample sentence\n",
    "        features = self.process_sentence(sentence)\n",
    "        for lemma in features:\n",
    "            if all(len(get_close_matches(word, unique_lemmas)) > 0 for word in lemma.split(\" \")):\n",
    "                return features[lemma]\n",
    "            \n",
    "        print(\"Missing lexical unit {} in sentence {}\".format(lu, sentence))\n",
    "        \n",
    "        return [None, None, None]\n",
    "\n",
    "    def predict_frame(self, lu, processed_sentence):\n",
    "        if lu in self.rules.keys():\n",
    "            # probs = self.rules[lu]\n",
    "            # pred_frame = np.random.choice(list(probs.keys()), p = list(probs.values()))\n",
    "            return self.rules[lu]\n",
    "        elif lu in self.models.keys():\n",
    "            features = [processed_sentence[\"_\".join(lu.split(\"_\")[:-1])]]\n",
    "            X = pd.DataFrame(features, columns = [\"location\", \"relation\", \"head\"])\n",
    "            return self.models[lu].predict(X)[0]\n",
    "        else:\n",
    "            raise Exception(\"Unknown lexical unit: {}\".format(lu))\n",
    "    \n",
    "    def fit(self, df_train = None, output_dir = \"../models/lexical_units\", random_state = None):\n",
    "        # Delete old pkl files in output_dir\n",
    "        for file in os.listdir(output_dir):\n",
    "            if file.endswith(\".pkl\"):\n",
    "                os.remove(os.path.join(output_dir, file))\n",
    "        \n",
    "        if df_train is not None:\n",
    "            self.df = copy.deepcopy(df_train)\n",
    "        elif self.df is None:\n",
    "            self.load_training_data()\n",
    "            \n",
    "        self.rules = {}\n",
    "        self.models = {}\n",
    "        for lu, frames in self.lu_frames.items():\n",
    "            if len(frames) > 1:\n",
    "                df_lu = self.df[self.df[\"Lexical Unit\"] == lu]\n",
    "                frame_counts = df_lu.groupby(\"Frame\")[\"Sentence\"].count()\n",
    "                \n",
    "                if min(frame_counts) < 10:\n",
    "                    frame_counts = frame_counts / sum(frame_counts)\n",
    "                    self.rules[lu] = frame_counts.to_dict()\n",
    "                else:\n",
    "                    df_lu_no_na = df_lu.dropna().reset_index(drop = True)\n",
    "                    features = list(map(lambda x: self.match_lemmas(x, lu), df_lu_no_na[\"Sentence\"]))\n",
    "                    X = pd.DataFrame(features, columns = [\"location\", \"relation\", \"head\"])\n",
    "                    X[\"Frame\"] = df_lu_no_na[\"Frame\"]\n",
    "                    X.dropna(inplace = True)\n",
    "                    frame_labels = X[\"Frame\"]\n",
    "                    X.drop(\"Frame\", axis = 1, inplace = True)\n",
    "                    \n",
    "                    if len(X) < 10 * len(self.lu_frames[lu]):\n",
    "                        frame_counts = frame_counts / sum(frame_counts)\n",
    "                        self.rules[lu] = frame_counts.to_dict()\n",
    "                    else:\n",
    "                        cat_pipeline = Pipeline([\n",
    "                            (\"ohe\", OneHotEncoder(handle_unknown = \"ignore\"))\n",
    "                        ])\n",
    "                        col_transformer = ColumnTransformer([\n",
    "                            (\"cat\", cat_pipeline, [\"relation\", \"head\"])\n",
    "                        ])\n",
    "                        pipeline = Pipeline([\n",
    "                            (\"preprocessing\", col_transformer),\n",
    "                            (\"model\", DecisionTreeClassifier(random_state = random_state))\n",
    "                        ])\n",
    "                        pipeline.fit(X, frame_labels)\n",
    "                        pkl.dump(pipeline, open(\"{}/{}.pkl\".format(output_dir, lu), \"wb\"))\n",
    "                        self.models[lu] = pipeline\n",
    "                    \n",
    "        json.dump(self.rules, open(\"{}/rules.json\".format(output_dir), \"w\"), indent = 4)\n",
    "    \n",
    "    def predict(self, sentences, model_dir = None):\n",
    "        if not hasattr(self, \"models\") or self.models is None:\n",
    "            if model_dir is None:\n",
    "                self.load_trained_models()\n",
    "            else:\n",
    "                self.load_trained_models(model_dir)\n",
    "                \n",
    "        predictions = []\n",
    "        for sentence in sentences:\n",
    "            doc = self.nlp(sentence)\n",
    "            pos = self.pos_tag(sentence, doc)\n",
    "            processed_sentence = self.process_sentence(sentence, doc)\n",
    "            curr = []\n",
    "            for word, tag in pos:\n",
    "                lu = self.get_word_lu(word, tag)\n",
    "                if lu is not None:\n",
    "                    possible_frames = self.lu_frames[lu]\n",
    "                    if len(possible_frames) == 1:\n",
    "                        curr.append((lu, possible_frames[0]))\n",
    "                    else:\n",
    "                        curr.append((lu, self.predict_frame(lu, processed_sentence)))\n",
    "            predictions.append(curr)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/ryanschaefer/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "model = LexicalUnitClassifier(True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And now she took a better look at him , Folly could n't help noticing the strong , muscular lines of the broad back under that white shirt .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('now_adv', 'Temporal_collocation'),\n",
       " ('better_v', 'Surpassing'),\n",
       " ('look_n', 'Perception_active'),\n",
       " ('at_prep', 'Spatial_co-location'),\n",
       " ('could_v', 'Possibility'),\n",
       " ('help_v', 'Assistance'),\n",
       " ('strong_a', 'Level_of_force_resistance'),\n",
       " ('muscular_a', 'Body_description_holistic'),\n",
       " ('of_prep', 'Partitive'),\n",
       " ('broad_a', 'Dimension'),\n",
       " ('back_n', 'Body_parts'),\n",
       " ('under_prep', 'Non-gradable_proximity'),\n",
       " ('that_adv', 'Degree'),\n",
       " ('white_a', 'Color'),\n",
       " ('shirt_n', 'Clothing')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [df[\"Sentence\"][1]]\n",
    "predicted_frames = model.predict(sentences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    display(predicted_frames[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
