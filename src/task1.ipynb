{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ryanschaefer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/ryanschaefer/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import framenet as fn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88640 entries, 0 to 88639\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Lexical Unit    88640 non-null  object\n",
      " 1   Frame Count     88640 non-null  int64 \n",
      " 2   Frame           88640 non-null  object\n",
      " 3   Sentence Count  88640 non-null  int64 \n",
      " 4   Sentence        87409 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 3.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frame Count</th>\n",
       "      <th>Sentence Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>88640.000000</td>\n",
       "      <td>88640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.203847</td>\n",
       "      <td>57.749492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.781488</td>\n",
       "      <td>59.231118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>401.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Frame Count  Sentence Count\n",
       "count  88640.000000    88640.000000\n",
       "mean       3.203847       57.749492\n",
       "std        1.781488       59.231118\n",
       "min        2.000000        0.000000\n",
       "25%        2.000000       21.000000\n",
       "50%        2.000000       39.000000\n",
       "75%        4.000000       73.000000\n",
       "max       11.000000      401.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexical Unit</th>\n",
       "      <th>Frame Count</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Sentence Count</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faith.n</td>\n",
       "      <td>2</td>\n",
       "      <td>Religious_belief</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faith.n</td>\n",
       "      <td>2</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>Legend has it that a local woman climbed the h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>degree.n</td>\n",
       "      <td>3</td>\n",
       "      <td>Quantity</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>degree.n</td>\n",
       "      <td>3</td>\n",
       "      <td>Quantified_mass</td>\n",
       "      <td>29</td>\n",
       "      <td>Specialist labour or industrial correspondents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>degree.n</td>\n",
       "      <td>3</td>\n",
       "      <td>Quantified_mass</td>\n",
       "      <td>29</td>\n",
       "      <td>The incremental approach has also been known t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88635</th>\n",
       "      <td>evacuate.v</td>\n",
       "      <td>4</td>\n",
       "      <td>Emptying</td>\n",
       "      <td>6</td>\n",
       "      <td>Some nearby buildings have also been evacuated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88636</th>\n",
       "      <td>evacuate.v</td>\n",
       "      <td>4</td>\n",
       "      <td>Emptying</td>\n",
       "      <td>6</td>\n",
       "      <td>The government of the Maldives has decided to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88637</th>\n",
       "      <td>evacuate.v</td>\n",
       "      <td>4</td>\n",
       "      <td>Emptying</td>\n",
       "      <td>6</td>\n",
       "      <td>Let us assume you wish to evacuate the nightcl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88638</th>\n",
       "      <td>evacuate.v</td>\n",
       "      <td>4</td>\n",
       "      <td>Emptying</td>\n",
       "      <td>6</td>\n",
       "      <td>The fire brigade reappeared , bringing them so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88639</th>\n",
       "      <td>evacuate.v</td>\n",
       "      <td>4</td>\n",
       "      <td>Emptying</td>\n",
       "      <td>6</td>\n",
       "      <td>Dense black smoke billowed 3,000ft into the ai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88640 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lexical Unit  Frame Count             Frame  Sentence Count  \\\n",
       "0          faith.n            2  Religious_belief               0   \n",
       "1          faith.n            2             Trust               1   \n",
       "2         degree.n            3          Quantity               0   \n",
       "3         degree.n            3   Quantified_mass              29   \n",
       "4         degree.n            3   Quantified_mass              29   \n",
       "...            ...          ...               ...             ...   \n",
       "88635   evacuate.v            4          Emptying               6   \n",
       "88636   evacuate.v            4          Emptying               6   \n",
       "88637   evacuate.v            4          Emptying               6   \n",
       "88638   evacuate.v            4          Emptying               6   \n",
       "88639   evacuate.v            4          Emptying               6   \n",
       "\n",
       "                                                Sentence  \n",
       "0                                                    NaN  \n",
       "1      Legend has it that a local woman climbed the h...  \n",
       "2                                                    NaN  \n",
       "3      Specialist labour or industrial correspondents...  \n",
       "4      The incremental approach has also been known t...  \n",
       "...                                                  ...  \n",
       "88635  Some nearby buildings have also been evacuated...  \n",
       "88636  The government of the Maldives has decided to ...  \n",
       "88637  Let us assume you wish to evacuate the nightcl...  \n",
       "88638  The fire brigade reappeared , bringing them so...  \n",
       "88639  Dense black smoke billowed 3,000ft into the ai...  \n",
       "\n",
       "[88640 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/lexical_unit_sentences.csv\")\n",
    "\n",
    "display(df.info())\n",
    "display(df.describe())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Religious_belief': 0, 'Trust': 1}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Lexical Unit\"] == \"faith.n\"].groupby(\"Frame\")[\"Sentence\"].count().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Religious_belief': 0.0, 'Trust': 1.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = df[df[\"Lexical Unit\"] == \"faith.n\"].groupby(\"Frame\")[\"Sentence\"].count()\n",
    "tmp = tmp / sum(tmp)\n",
    "tmp.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyLexicalUnits:\n",
    "    def __init__(\n",
    "        self, load_training = True, pretrained = True, \n",
    "        training_filename = \"../datasets/lexical_unit_sentences.csv\",\n",
    "        model_directory = \"../models/lexical_units\"\n",
    "    ):\n",
    "        self.df = None\n",
    "        self.models = None\n",
    "        self.rules = None\n",
    "        self.load_framenet()\n",
    "    \n",
    "        if load_training:\n",
    "            self.load_training_data(training_filename)\n",
    "            \n",
    "        if pretrained:\n",
    "            self.load_trained_models(model_directory)\n",
    "            \n",
    "    def load_framenet(self):\n",
    "        nltk.download(\"framenet_v17\")\n",
    "        lexical_units = fn.lus()\n",
    "        lu_names = list(set(map(lambda x: x.name.replace(\".\", \"_\"), lexical_units)))\n",
    "        self.lu_frames = { key: [] for key in lu_names }\n",
    "        for lu in lexical_units:\n",
    "            name = str.replace(lu.name, \".\", \"_\")\n",
    "            if name not in self.lu_frames[name]:\n",
    "                self.lu_frames[name].append(lu.frame.name)\n",
    "                self.lu_frames[name] = list(sorted(self.lu_frames[name]))\n",
    "    \n",
    "    def load_training_data(self, filename = \"../datasets/lexical_unit_sentences.csv\"):\n",
    "        self.df = pd.read_csv(filename)\n",
    "        self.df[\"POS\"] = self.df[\"Sentence\"].apply(self.pos_tag)\n",
    "        self.df[\"Lexical Unit\"] = self.df[\"Lexical Unit\"].str.replace(\".\", \"_\")\n",
    "        \n",
    "    def load_trained_models(self, directory = \"../models/lexical_units\"):\n",
    "        self.models = {}\n",
    "        self.rules = json.load(open(\"{}/rules.json\".format(directory)))\n",
    "        for filename in os.listdir(directory):\n",
    "            comps = filename.split(\".\")\n",
    "            if comps[1] == \"pkl\":\n",
    "                self.models[comps[0]] = pkl.load(open(os.join(directory, filename), \"rb\"))\n",
    "                \n",
    "    def pos_tag(self, sentence):\n",
    "        if type(sentence) != str:\n",
    "            return []\n",
    "        else:\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            base_tags = nltk.pos_tag(tokens)\n",
    "            final_tags = []\n",
    "            for word, tag in base_tags:\n",
    "                if tag.startswith(\"N\"):\n",
    "                    final_tags.append((word, \"_n\"))\n",
    "                elif tag.startswith(\"J\"):\n",
    "                    final_tags.append((word, \"_a\"))\n",
    "                elif tag.startswith(\"V\"):\n",
    "                    final_tags.append((word, \"_v\"))\n",
    "                elif tag.startswith(\"R\"):\n",
    "                    final_tags.append((word, \"_adv\"))\n",
    "                elif tag == \"IN\":\n",
    "                    final_tags.append((word, \"_prep\"))\n",
    "                elif tag == \"CD\":\n",
    "                    final_tags.append((word, \"_num\"))\n",
    "                elif tag == \"CC\":\n",
    "                    final_tags.append((word, \"_c\"))\n",
    "                elif tag == \"UH\":\n",
    "                    final_tags.append((word, \"_intj\"))\n",
    "                elif tag == \"DT\":\n",
    "                    final_tags.append((word, \"_art\"))\n",
    "                else:\n",
    "                    final_tags.append((word, \"_scon\"))\n",
    "                    \n",
    "            return final_tags\n",
    "            \n",
    "        \n",
    "    def get_word_lu(self, word, pos):\n",
    "        possible_lus = list(filter(lambda x: x.startswith(\"{}\".format(word.lower())), self.lu_frames.keys()))\n",
    "        if len(possible_lus) == 0:\n",
    "            return None\n",
    "        elif len(possible_lus) == 1:\n",
    "            return possible_lus[0]\n",
    "        else:\n",
    "            tmp = word + pos\n",
    "            if tmp in possible_lus:\n",
    "                return tmp\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "    def process_sentence(self, sentence):\n",
    "        pass\n",
    "    \n",
    "    def predict_frame(self, lu, sentence):\n",
    "        if lu in self.rules.keys():\n",
    "            probs = self.rules[lu]\n",
    "            pred_frame = np.random.choice(list(probs.keys()), p = list(probs.values()))\n",
    "            return pred_frame\n",
    "        elif lu in self.models.keys():\n",
    "            features = [self.process_sentence(sentence)]\n",
    "            return self.models[lu].predict(features)[0]\n",
    "        else:\n",
    "            raise Exception(\"Unknown lexical unit: {}\".format(lu))\n",
    "    \n",
    "    def fit(self, df_train = None, output_dir = \"../models/lexical_units\", random_state = None):\n",
    "        if df_train is not None:\n",
    "            self.df = copy.deepcopy(df_train)\n",
    "            self.df[\"POS\"] = self.df[\"Sentence\"].apply(self.pos_tag)\n",
    "        elif self.df is None:\n",
    "            self.load_training_data()\n",
    "            \n",
    "        self.rules = {}\n",
    "        self.models = {}\n",
    "        for lu, frames in self.lu_frames.items():\n",
    "            if len(frames) > 1:\n",
    "                df_lu = self.df[self.df[\"Lexical Unit\"] == lu]\n",
    "                frame_counts = df_lu.groupby(\"Frame\")[\"Sentence\"].count()\n",
    "                if min(frame_counts) < 10:\n",
    "                    frame_counts = frame_counts / sum(frame_counts)\n",
    "                    self.rules[lu] = frame_counts.to_dict()\n",
    "                else:\n",
    "                    features = df_lu[\"Sentence\"].apply(self.process_sentence).to_numpy()\n",
    "                    model = DecisionTreeClassifier(random_state = random_state)\n",
    "                    model.fit(features, df_lu[\"Frame\"])\n",
    "                    pkl.dump(model, open(\"{}/{}.pkl\".format(output_dir, lu), \"wb\"))\n",
    "                    self.models[lu] = model\n",
    "                    \n",
    "        json.dump(self.rules, open(\"{}/rules.json\".format(output_dir), \"w\"), indent = 4)\n",
    "    \n",
    "    def predict(self, sentences, model_dir = None):\n",
    "        if self.models is None:\n",
    "            if model_dir is None:\n",
    "                self.load_trained_models()\n",
    "            else:\n",
    "                self.load_trained_models(model_dir)\n",
    "                \n",
    "        pos = list(map(self.pos_tag, sentences))\n",
    "        predictions = []\n",
    "        for i in range(len(sentences)):\n",
    "            curr = []\n",
    "            for word, tag in pos[i]:\n",
    "                lu = self.get_word_lu(word, tag)\n",
    "                if lu is not None:\n",
    "                    possible_frames = self.lu_frames[lu]\n",
    "                    if len(possible_frames) == 1:\n",
    "                        curr.append((lu, possible_frames[0]))\n",
    "                    else:\n",
    "                        curr.append((lu, self.predict_frame(lu, sentences[i])))\n",
    "            predictions.append(curr)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package framenet_v17 to\n",
      "[nltk_data]     /Users/ryanschaefer/nltk_data...\n",
      "[nltk_data]   Package framenet_v17 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "clu = ClassifyLexicalUnits(False)\n",
    "\n",
    "print(len(clu.rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m large \u001b[38;5;241m=\u001b[39m \u001b[43mclu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRules: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Large: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(clu\u001b[38;5;241m.\u001b[39mrules), \u001b[38;5;28mlen\u001b[39m(large)))\n",
      "Cell \u001b[0;32mIn[36], line 100\u001b[0m, in \u001b[0;36mClassifyLexicalUnits.fit\u001b[0;34m(self, df_train, output_dir)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_tag)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrules \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[36], line 31\u001b[0m, in \u001b[0;36mClassifyLexicalUnits.load_training_data\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_training_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets/lexical_unit_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(filename)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLexical Unit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLexical Unit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[36], line 47\u001b[0m, in \u001b[0;36mClassifyLexicalUnits.pos_tag\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(sentence)\n\u001b[0;32m---> 47\u001b[0m     base_tags \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     final_tags \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m base_tags:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.8/site-packages/nltk/tag/__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.8/site-packages/nltk/tag/__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.8/site-packages/nltk/tag/perceptron.py:187\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tag:\n\u001b[1;32m    186\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(i, word, context, prev, prev2)\n\u001b[0;32m--> 187\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m output\u001b[38;5;241m.\u001b[39mappend((word, tag, conf) \u001b[38;5;28;01mif\u001b[39;00m return_conf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (word, tag))\n\u001b[1;32m    190\u001b[0m prev2 \u001b[38;5;241m=\u001b[39m prev\n",
      "File \u001b[0;32m~/miniconda3/envs/mlenv/lib/python3.8/site-packages/nltk/tag/perceptron.py:66\u001b[0m, in \u001b[0;36mAveragedPerceptron.predict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     64\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[feat]\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label, weight \u001b[38;5;129;01min\u001b[39;00m weights\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 66\u001b[0m         scores[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Do a secondary alphabetic sort, for stability\u001b[39;00m\n\u001b[1;32m     69\u001b[0m best_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m label: (scores[label], label))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "large = clu.fit()\n",
    "\n",
    "print(\"Rules: {}, Large: {}\".format(len(clu.rules), len(large)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_n chose Calendric_unit\n",
      "watch_v chose Perception_active\n",
      "for_prep chose Duration_relation\n",
      "from_prep chose Time_vector\n",
      "day_n chose Calendric_unit\n",
      "child_n chose People_by_age\n",
      "stone_v chose Cause_harm\n",
      "of_prep chose Partitive\n",
      "faith_n chose Trust\n",
      "Legend has it that a local woman climbed the hill every day to watch for her husband returning from across the sea ; one day the wife and her child were turned to stone as a permanent symbol of her enduring faith .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('legendary_a', 'Fame'),\n",
       " ('that_adv', 'Degree'),\n",
       " ('local_a', 'Political_locales'),\n",
       " ('woman_n', 'People'),\n",
       " ('hill_n', 'Natural_features'),\n",
       " ('day_n', 'Calendric_unit'),\n",
       " ('watch_v', 'Perception_active'),\n",
       " ('for_prep', 'Duration_relation'),\n",
       " ('husband_n', 'Personal_relationship'),\n",
       " ('from_prep', 'Time_vector'),\n",
       " ('across_prep', 'Distributed_position'),\n",
       " ('sea_n', 'Natural_features'),\n",
       " ('one_num', 'Cardinal_numbers'),\n",
       " ('day_n', 'Calendric_unit'),\n",
       " ('wife_n', 'Personal_relationship'),\n",
       " ('child_n', 'People_by_age'),\n",
       " ('turned on_a', 'Biological_urge'),\n",
       " ('stone_v', 'Cause_harm'),\n",
       " ('as_prep', 'Performers_and_roles'),\n",
       " ('symbolize_v', 'Representing'),\n",
       " ('of_prep', 'Partitive'),\n",
       " ('enduring_a', 'Duration_description'),\n",
       " ('faith_n', 'Trust')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [df[\"Sentence\"][1]]\n",
    "predicted_frames = clu.predict(sentences)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    display(predicted_frames[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
